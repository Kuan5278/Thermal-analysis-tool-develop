def generate_flexible_chart(df, left_col, right_col, x_limits, y_limits=None):
    if df is None or not left_col or left_col not in df.columns: return None
    if right_col and right_col != 'None' and right_col not in df.columns: return None
    
    df_chart = df.copy()
    if x_limits:
        x_min_td, x_max_td = pd.to_timedelta(x_limits[0], unit='s'), pd.to_timedelta(x_limits[1], unit='s')
        df_chart = df_chart[(df_chart.index >= x_min_td) & (df_chart.index <= x_max_td)]
    if df_chart.empty: return None
    
    df_chart.loc[:, 'left_val'] = pd.to_numeric(df_chart[left_col], errors='coerce')
    if right_col and right_col != 'None':
        df_chart.loc[:, 'right_val'] = pd.to_numeric(df_chart[right_col], errors='coerce')
    
    fig, ax1 = plt.subplots(figsize=(10.2, 5.1))
    plt.title(f'{left_col} {"& " + right_col if right_col and right_col != "None" else ""}', fontsize=14, fontweight='bold')
    
    x_axis_seconds = df_chart.index.total_seconds()
    color = 'tab:blue'
    ax1.set_xlabel('Elapsed Time (seconds)', fontsize=11)
    ax1.set_ylabel(left_col, color=color, fontsize=11)
    ax1.plot(x_axis_seconds, df_chart['left_val'], color=color, linewidth=1.5)
    ax1.tick_params(axis='y', labelcolor=color)
    ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    
    if y_limits:
        ax1.set_ylim(y_limits)
    
    if right_col and right_col != 'None':
        ax2 = ax1.twinx()
        color = 'tab:red'
        ax2.set_ylabel(right_col, color=color, fontsize=11)
        ax2.plot(x_axis_seconds, df_chart['right_val'], color=color, linewidth=1.5)
        ax2.tick_params(axis='y', labelcolor=color)
    
    if x_limits: 
        ax1.set_xlim(x_limits)
    
    fig.tight_layout()
    return fig

# --- ç‰ˆæœ¬è³‡è¨Šé¡¯ç¤ºå‡½å¼ ---
def display_version_info():
    """é¡¯ç¤ºç‰ˆæœ¬è³‡è¨Š"""
    with st.expander("ğŸ“‹ ç‰ˆæœ¬è³‡è¨Š", expanded=False):
        st.markdown(f"""
        **ç•¶å‰ç‰ˆæœ¬ï¼š{VERSION}** | **ç™¼å¸ƒæ—¥æœŸï¼š{VERSION_DATE}**
        
        ### ğŸ†• æœ¬ç‰ˆæœ¬æ›´æ–°å…§å®¹ï¼š
        """)
        
        for feature in VERSION_FEATURES:
            st.markdown(f"- {feature}")
        
        st.markdown("---")
        st.markdown("ğŸ’¡ **ä½¿ç”¨æç¤ºï¼š** æ”¯æ´YOKOGAWA Excelæ ¼å¼ã€PTAT CSVæ ¼å¼ï¼Œç¾åœ¨åŠ å…¥AIåŠŸèƒ½ï¼")

# --- ä¸»æ‡‰ç”¨ç¨‹å¼ ---
def main():
    st.set_page_config(
        page_title="AI-Ready æ•¸æ“šåˆ†æå¹³å°",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # åˆå§‹åŒ–AIçµ„ä»¶
    ai_engine = BasicAIEngine()
    conversation_system = SimpleConversationSystem()
    
    # Session State åˆå§‹åŒ–
    if 'ai_analysis_results' not in st.session_state:
        st.session_state.ai_analysis_results = []
    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
    
    # è‡ªå®šç¾©CSSæ¨£å¼
    st.markdown("""
    <style>
        .main-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
            padding: 1.5rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            text-align: center;
            color: white;
        }
        .ai-feature-box {
            background: linear-gradient(145deg, #e3f2fd, #bbdefb);
            border: 2px solid #2196f3;
            border-radius: 10px;
            padding: 1rem;
            margin: 1rem 0;
        }
        .metric-card {
            background-color: #f8f9fa;
            padding: 1rem;
            border-radius: 8px;
            border-left: 4px solid #667eea;
            margin: 0.5rem 0;
        }
        .success-box {
            background-color: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
            padding: 1rem;
            border-radius: 8px;
            margin: 1rem 0;
        }
        .info-box {
            background-color: #d1ecf1;
            border: 1px solid #bee5eb;
            color: #0c5460;
            padding: 1rem;
            border-radius: 8px;
            margin: 1rem 0;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # ä¸»é é¢æ¨™é¡Œ
    st.markdown(f"""
    <div class="main-header">
        <h1>ğŸ¤– AI-Ready æ•¸æ“šåˆ†æå¹³å°</h1>
        <p>æ™ºèƒ½è§£æ YOKOGAWA & PTAT Log æ–‡ä»¶ï¼Œç¾åœ¨åŠ å…¥AIåŠŸèƒ½ï¼</p>
        <p><strong>{VERSION}</strong> | {VERSION_DATE}</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ç‰ˆæœ¬è³‡è¨Šå€åŸŸ
    display_version_info()
    
    # AIåŠŸèƒ½ç‹€æ…‹é¡¯ç¤º
    with st.expander("ğŸ¤– AIåŠŸèƒ½ç‹€æ…‹", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            if AI_ANOMALY_AVAILABLE:
                st.success("âœ… MLç•°å¸¸æª¢æ¸¬ å·²å•Ÿç”¨")
                st.info("ğŸ”¬ ä½¿ç”¨ Isolation Forest ç®—æ³•")
            else:
                st.warning("âš ï¸ MLç•°å¸¸æª¢æ¸¬ æœªå•Ÿç”¨")
                st.info("ğŸ’¡ å®‰è£… scikit-learn å•Ÿç”¨é«˜ç´šåŠŸèƒ½")
        
        with col2:
            if PLOTLY_AVAILABLE:
                st.success("âœ… äº¤äº’å¼åœ–è¡¨ å·²å•Ÿç”¨")
                st.info("ğŸ“Š æ”¯æ´ Plotly åœ–è¡¨")
            else:
                st.warning("âš ï¸ äº¤äº’å¼åœ–è¡¨ æœªå•Ÿç”¨")
                st.info("ğŸ’¡ å®‰è£… plotly å•Ÿç”¨äº¤äº’åŠŸèƒ½")
        
        with col3:
            if SCIPY_AVAILABLE:
                st.success("âœ… é«˜ç´šçµ±è¨ˆ å·²å•Ÿç”¨")
                st.info("ğŸ“ˆ æ”¯æ´çµ±è¨ˆæª¢é©—")
            else:
                st.warning("âš ï¸ é«˜ç´šçµ±è¨ˆ æœªå•Ÿç”¨")
                st.info("ğŸ’¡ å®‰è£… scipy å•Ÿç”¨çµ±è¨ˆåŠŸèƒ½")
    
    # å´é‚Šæ¬„è¨­è¨ˆ
    st.sidebar.markdown("### ğŸ›ï¸ æ§åˆ¶é¢æ¿")
    st.sidebar.markdown("---")
    
    uploaded_files = st.sidebar.file_uploader(
        "ğŸ“ ä¸Šå‚³Log File (å¯å¤šé¸)", 
        type=['csv', 'xlsx'], 
        accept_multiple_files=True,
        help="æ”¯æ´ YOKOGAWA Excel æ ¼å¼å’Œ PTAT CSV æ ¼å¼"
    )
    
    # AIåŠŸèƒ½é¸æ“‡
    st.sidebar.markdown("### ğŸ¤– AIåŠŸèƒ½é¸æ“‡")
    ai_features = st.sidebar.multiselect(
        "é¸æ“‡AIåˆ†æåŠŸèƒ½",
        ["ğŸ” åŸºç¤ç•°å¸¸æª¢æ¸¬", "ğŸ¤– MLç•°å¸¸æª¢æ¸¬", "ğŸ“ˆ è¶¨å‹¢åˆ†æ", "ğŸ’¡ æ™ºèƒ½æ´å¯Ÿ"],
        default=["ğŸ” åŸºç¤ç•°å¸¸æª¢æ¸¬", "ğŸ“ˆ è¶¨å‹¢åˆ†æ"]
    )
    
    # æ™ºèƒ½å°è©±å€åŸŸ
    st.sidebar.markdown("### ğŸ’¬ AIåŠ©æ‰‹")
    user_query = st.sidebar.text_input(
        "å•AIåŠ©æ‰‹",
        placeholder="ä¾‹å¦‚ï¼šæª¢æ¸¬ç•°å¸¸ã€åˆ†æè¶¨å‹¢...",
        key="ai_query"
    )
    
    if user_query:
        ai_response = conversation_system.process_query(user_query)
        st.sidebar.markdown(f"""
        <div class="ai-feature-box">
            <strong>ğŸ¤– AIå›æ‡‰ï¼š</strong><br>
            {ai_response}
        </div>
        """, unsafe_allow_html=True)
    
    if uploaded_files:
        # æª”æ¡ˆè³‡è¨Šé¡¯ç¤º
        st.sidebar.markdown("### ğŸ“‚ å·²ä¸Šå‚³æª”æ¡ˆ")
        for i, file in enumerate(uploaded_files, 1):
            file_size = len(file.getvalue()) / 1024
            st.sidebar.markdown(f"**{i}.** `{file.name}` ({file_size:.1f} KB)")
        
        st.sidebar.markdown("---")
        
        # æª”æ¡ˆè§£æ
        if len(uploaded_files) == 1:
            df_check, log_type_check = parse_dispatcher(uploaded_files[0])
            is_single_yokogawa = (log_type_check == "YOKOGAWA Log")
        else:
            is_single_yokogawa = False
        
        # å‰µå»ºé é¢é¸é …å¡
        if is_single_yokogawa:
            tab1, tab2, tab3 = st.tabs(["ğŸ“Š æ•¸æ“šåˆ†æ", "ğŸ¤– AIåˆ†æ", "ğŸ“ˆ äº¤äº’åœ–è¡¨"])
        else:
            tab1, tab2, tab3 = st.tabs(["ğŸ“Š æ•¸æ“šåˆ†æ", "ğŸ¤– AIåˆ†æ", "ğŸ“ˆ äº¤äº’åœ–è¡¨"])
        
        with tab1:
            # åŸæœ‰çš„æ•¸æ“šåˆ†æåŠŸèƒ½
            if is_single_yokogawa:
                st.markdown(f"""
                <div class="success-box">
                    <strong>âœ… æª”æ¡ˆè§£ææˆåŠŸ</strong><br>
                    ğŸ“„ æª”æ¡ˆé¡å‹ï¼š{log_type_check}<br>
                    ğŸ“Š æ•¸æ“šç­†æ•¸ï¼š{len(df_check):,} ç­†<br>
                    ğŸ”¢ é€šé“æ•¸é‡ï¼š{len([c for c in df_check.columns if df_check[c].dtype in ['float64', 'int64']]):,} å€‹
                </div>
                """, unsafe_allow_html=True)
                
                # åœ–è¡¨è¨­å®š
                st.sidebar.markdown("### âš™ï¸ åœ–è¡¨è¨­å®š")
                
                if df_check is not None and len(df_check) > 0:
                    x_min_val = df_check.index.min().total_seconds()
                    x_max_val = df_check.index.max().total_seconds()
                    
                    if x_min_val < x_max_val:
                        x_min, x_max = st.sidebar.slider(
                            "â±ï¸ æ™‚é–“ç¯„åœ (ç§’)", 
                            float(x_min_val), 
                            float(x_max_val), 
                            (float(x_min_val), float(x_max_val)),
                            key="yokogawa_time_range"
                        )
                        x_limits = (x_min, x_max)
                    else:
                        x_limits = None
                    
                    # Yè»¸ç¯„åœè¨­å®š
                    st.sidebar.markdown("#### ğŸ¯ Yè»¸æº«åº¦ç¯„åœ")
                    df_temp = df_check.copy()
                    if x_limits:
                        x_min_td = pd.to_timedelta(x_limits[0], unit='s')
                        x_max_td = pd.to_timedelta(x_limits[1], unit='s')
                        df_temp = df_temp[(df_temp.index >= x_min_td) & (df_temp.index <= x_max_td)]
                    
                    if not df_temp.empty:
                        numeric_cols = df_temp.select_dtypes(include=['number']).columns
                        temp_cols = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME']]
                        
                        if temp_cols:
                            all_temps = pd.concat([pd.to_numeric(df_temp[col], errors='coerce') for col in temp_cols])
                            all_temps = all_temps.dropna()
                            
                            if len(all_temps) > 0:
                                temp_min = float(all_temps.min())
                                temp_max = float(all_temps.max())
                                temp_range = temp_max - temp_min
                                buffer = temp_range * 0.1 if temp_range > 0 else 5
                                
                                auto_y_range = st.sidebar.checkbox("ğŸ”„ è‡ªå‹•Yè»¸ç¯„åœ", value=True)
                                
                                if not auto_y_range:
                                    y_min, y_max = st.sidebar.slider(
                                        "ğŸŒ¡ï¸ æº«åº¦ç¯„åœ (Â°C)",
                                        temp_min - buffer,
                                        temp_max + buffer,
                                        (temp_min - buffer, temp_max + buffer),
                                        step=0.1,
                                        key="yokogawa_y_range"
                                    )
                                    y_limits = (y_min, y_max)
                                else:
                                    y_limits = None
                                
                                st.sidebar.markdown(f"""
                                **ğŸ“ˆ ç•¶å‰æº«åº¦ç¯„åœï¼š**
                                - æœ€é«˜ï¼š{temp_max:.1f}Â°C
                                - æœ€ä½ï¼š{temp_min:.1f}Â°C
                                - å·®å€¼ï¼š{temp_range:.1f}Â°C
                                """)
                            else:
                                y_limits = None
                        else:
                            y_limits = None
                    else:
                        y_limits = None
                else:
                    x_limits = None
                    y_limits = None
                
                # ä¸»è¦å…§å®¹å€åŸŸ
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown("### ğŸ“ˆ YOKOGAWA å…¨é€šé“æº«åº¦æ›²ç·šåœ–")
                    
                    if df_check is not None:
                        fig = generate_yokogawa_temp_chart(df_check, x_limits, y_limits)
                        if fig: 
                            st.pyplot(fig)
                        else: 
                            st.warning("âš ï¸ ç„¡æ³•ç”¢ç”Ÿæº«åº¦åœ–è¡¨")
                    else:
                        st.error("âŒ æ•¸æ“šè§£æå¤±æ•—")
                
                with col2:
                    st.markdown("### ğŸ“Š çµ±è¨ˆæ•¸æ“š")
                    stats_df = calculate_temp_stats(df_check, x_limits)
                    if not stats_df.empty:
                        st.dataframe(stats_df, use_container_width=True, hide_index=True)
                        
                        if len(stats_df) > 0:
                            try:
                                max_temps = [float(x.replace('Â°C', '')) for x in stats_df['Tmax (Â°C)'] if x != 'N/A']
                                avg_temps = [float(x.replace('Â°C', '')) for x in stats_df['Tavg (Â°C)'] if x != 'N/A']
                                
                                if max_temps and avg_temps:
                                    st.markdown(f"""
                                    <div class="metric-card">
                                        <strong>ğŸ”¥ æ•´é«”æœ€é«˜æº«ï¼š</strong> {max(max_temps):.1f}Â°C<br>
                                        <strong>ğŸ“Š å¹³å‡æº«åº¦ï¼š</strong> {sum(avg_temps)/len(avg_temps):.1f}Â°C<br>
                                        <strong>ğŸ“ˆ æ´»èºé€šé“ï¼š</strong> {len(stats_df)} å€‹
                                    </div>
                                    """, unsafe_allow_html=True)
                            except:
                                pass
                    else:
                        st.markdown("""
                        <div class="info-box">
                            â“ ç„¡çµ±è¨ˆæ•¸æ“šå¯é¡¯ç¤º<br>
                            è«‹æª¢æŸ¥æ™‚é–“ç¯„åœè¨­å®š
                        </div>
                        """, unsafe_allow_html=True)
            
            else:
                # è™•ç†PTATæˆ–å¤šæª”æ¡ˆ
                all_dfs = []
                log_types = []
                
                for file in uploaded_files:
                    df, log_type = parse_dispatcher(file)
                    if df is not None:
                        all_dfs.append(df)
                        log_types.append(log_type)
                
                if all_dfs:
                    # æª”æ¡ˆè§£æç‹€æ…‹
                    st.markdown("### ğŸ“‹ æª”æ¡ˆè§£æç‹€æ…‹")
                    status_cols = st.columns(len(uploaded_files))
                    
                    for i, (file, log_type) in enumerate(zip(uploaded_files, log_types)):
                        with status_cols[i]:
                            if i < len(all_dfs):
                                st.markdown(f"""
                                <div class="success-box">
                                    <strong>âœ… {file.name}</strong><br>
                                    ğŸ“„ {log_type}<br>
                                    ğŸ“Š {len(all_dfs[i]):,} ç­†æ•¸æ“š
                                </div>
                                """, unsafe_allow_html=True)
                            else:
                                st.markdown(f"""
                                <div style="background-color: #f8d7da; border: 1px solid #f5c6cb; color: #721c24; padding: 1rem; border-radius: 8px;">
                                    <strong>âŒ {file.name}</strong><br>
                                    è§£æå¤±æ•—
                                </div>
                                """, unsafe_allow_html=True)
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰PTAT Log
                    has_ptat = any("PTAT" in log_type for log_type in log_types)
                    
                    if has_ptat and len(all_dfs) == 1:
                        # å–®ä¸€PTAT Logçš„ç‰¹æ®Šè™•ç†
                        ptat_df = all_dfs[0]
                        
                        st.sidebar.markdown("### âš™ï¸ PTAT åœ–è¡¨è¨­å®š")
                        
                        # æ™‚é–“ç¯„åœè¨­å®š
                        if len(ptat_df) > 0:
                            x_min_val = ptat_df.index.min().total_seconds()
                            x_max_val = ptat_df.index.max().total_seconds()
                            
                            if x_min_val < x_max_val:
                                x_min, x_max = st.sidebar.slider(
                                    "â±ï¸ æ™‚é–“ç¯„åœ (ç§’)", 
                                    float(x_min_val), 
                                    float(x_max_val), 
                                    (float(x_min_val), float(x_max_val)),
                                    key="ptat_time_range"
                                )
                                x_limits = (x_min, x_max)
                            else:
                                x_limits = None
                        else:
                            x_limits = None
                        
                        # è®Šæ•¸é¸æ“‡
                        numeric_columns = ptat_df.select_dtypes(include=['number']).columns.tolist()
                        if numeric_columns:
                            st.sidebar.markdown("#### ğŸ¯ åƒæ•¸é¸æ“‡")
                            
                            default_left_list = [c for c in numeric_columns if 'Temp' in c or 'temperature' in c.lower()]
                            default_left = default_left_list[0] if default_left_list else numeric_columns[0]
                            left_y_axis = st.sidebar.selectbox(
                                "ğŸ“ˆ å·¦å´Yè»¸è®Šæ•¸", 
                                options=numeric_columns, 
                                index=numeric_columns.index(default_left) if default_left in numeric_columns else 0
                            )
                            
                            right_y_axis_options = ['None'] + numeric_columns
                            default_right_list = [c for c in numeric_columns if 'Power' in c or 'power' in c.lower()]
                            default_right = default_right_list[0] if default_right_list else 'None'
                            try: 
                                default_right_index = right_y_axis_options.index(default_right)
                            except ValueError: 
                                default_right_index = 0
                            right_y_axis = st.sidebar.selectbox(
                                "ğŸ“Š å³å´Yè»¸è®Šæ•¸ (å¯é¸)", 
                                options=right_y_axis_options, 
                                index=default_right_index
                            )
                            
                            # Yè»¸ç¯„åœè¨­å®š
                            st.sidebar.markdown("#### ğŸšï¸ Yè»¸ç¯„åœ")
                            auto_y = st.sidebar.checkbox("ğŸ”„ è‡ªå‹•Yè»¸ç¯„åœ", value=True)
                            y_limits = None
                            
                            if not auto_y and left_y_axis:
                                left_data = pd.to_numeric(ptat_df[left_y_axis], errors='coerce').dropna()
                                if len(left_data) > 0:
                                    data_min, data_max = float(left_data.min()), float(left_data.max())
                                    data_range = data_max - data_min
                                    buffer = data_range * 0.1 if data_range > 0 else 1
                                    y_min, y_max = st.sidebar.slider(
                                        f"ğŸ“Š {left_y_axis} ç¯„åœ",
                                        data_min - buffer,
                                        data_max + buffer,
                                        (data_min - buffer, data_max + buffer),
                                        step=0.1
                                    )
                                    y_limits = (y_min, y_max)
                            
                            # ä¸»è¦å…§å®¹å€åŸŸ
                            st.markdown("### ğŸ”¬ PTAT Log æ•¸æ“šåˆ†æ")
                            
                            # åœ–è¡¨é¡¯ç¤º
                            fig = generate_flexible_chart(ptat_df, left_y_axis, right_y_axis, x_limits, y_limits)
                            if fig: 
                                st.pyplot(fig, use_container_width=True)
                                
                                # PTAT Log å°ˆç”¨çµ±è¨ˆè¡¨æ ¼
                                st.markdown("### ğŸ“Š PTAT Log çµ±è¨ˆåˆ†æ")
                                
                                freq_df, power_df, temp_df = calculate_ptat_stats(ptat_df, x_limits)
                                
                                # ä½¿ç”¨ç¾åŒ–çš„åˆ†æ¬„å¸ƒå±€é¡¯ç¤ºä¸‰å€‹è¡¨æ ¼
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    st.markdown("#### ğŸ–¥ï¸ CPU Core Frequency")
                                    if freq_df is not None and not freq_df.empty:
                                        st.dataframe(freq_df, use_container_width=True, hide_index=True)
                                    else:
                                        st.markdown("""
                                        <div class="info-box">
                                            â“ æœªæ‰¾åˆ°CPUé »ç‡æ•¸æ“š
                                        </div>
                                        """, unsafe_allow_html=True)
                                
                                with col2:
                                    st.markdown("#### âš¡ Package Power")
                                    if power_df is not None and not power_df.empty:
                                        st.dataframe(power_df, use_container_width=True, hide_index=True)
                                    else:
                                        st.markdown("""
                                        <div class="info-box">
                                            â“ æœªæ‰¾åˆ°Package Poweræ•¸æ“š
                                        </div>
                                        """, unsafe_allow_html=True)
                                
                                with col3:
                                    st.markdown("#### ğŸŒ¡ï¸ MSR Package Temp")
                                    if temp_df is not None and not temp_df.empty:
                                        st.dataframe(temp_df, use_container_width=True, hide_index=True)
                                    else:
                                        st.markdown("""
                                        <div class="info-box">
                                            â“ æœªæ‰¾åˆ°MSR Package Temperatureæ•¸æ“š
                                        </div>
                                        """, unsafe_allow_html=True)
                            else:
                                st.warning("âš ï¸ ç„¡æ³•ç”¢ç”Ÿåœ–è¡¨")
                        else:
                            st.warning("âš ï¸ ç„¡å¯ç”¨çš„æ•¸å€¼å‹æ•¸æ“š")
        
        with tab2:
            st.markdown("### ğŸ¤– AIæ™ºèƒ½åˆ†æ")
            
            # é¸æ“‡åˆ†æçš„æ•¸æ“šé›†
            all_dfs = []
            log_types = []
            
            for file in uploaded_files:
                df, log_type = parse_dispatcher(file)
                if df is not None:
                    all_dfs.append(df)
                    log_types.append(log_type)
            
            if all_dfs:
                if len(all_dfs) > 1:
                    selected_df_idx = st.selectbox(
                        "é¸æ“‡æ•¸æ“šé›†é€²è¡ŒAIåˆ†æ",
                        range(len(all_dfs)),
                        format_func=lambda x: f"{uploaded_files[x].name} ({log_types[x]})"
                    )
                    current_df = all_dfs[selected_df_idx]
                else:
                    current_df = all_dfs[0]
                    selected_df_idx = 0
                
                # æ•¸æ“šæ¦‚æ³
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ•¸æ“šç­†æ•¸", f"{len(current_df):,}")
                with col2:
                    numeric_cols = current_df.select_dtypes(include=[np.number]).columns
                    st.metric("æ•¸å€¼æ¬„ä½", len(numeric_cols))
                with col3:
                    st.metric("AIåŠŸèƒ½", f"{len(ai_features)} å€‹å·²é¸")
                
                # AIåˆ†æåŸ·è¡Œå€åŸŸ
                st.markdown("#### ğŸ¤– åŸ·è¡ŒAIåˆ†æ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if "ğŸ” åŸºç¤ç•°å¸¸æª¢æ¸¬" in ai_features:
                        st.markdown("**ğŸ” åŸºç¤ç•°å¸¸æª¢æ¸¬**")
                        
                        # é¸æ“‡è¦æª¢æ¸¬çš„æ¬„ä½
                        detection_columns = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME']]
                        if detection_columns:
                            selected_col = st.selectbox(
                                "é¸æ“‡æª¢æ¸¬æ¬„ä½",
                                detection_columns,
                                key="basic_anomaly_col"
                            )
                            
                            threshold = st.slider(
                                "ç•°å¸¸é–¾å€¼å€æ•¸",
                                1.0, 5.0, 2.5, 0.1,
                                help="æ•¸å€¼è¶Šå¤§ï¼Œæª¢æ¸¬è¶Šåš´æ ¼",
                                key="basic_threshold"
                            )
                            
                            if st.button("ğŸ” åŸ·è¡ŒåŸºç¤ç•°å¸¸æª¢æ¸¬", key="basic_anomaly"):
                                with st.spinner("æ­£åœ¨åˆ†æ..."):
                                    result, msg = ai_engine.basic_anomaly_detection(
                                        current_df[selected_col], 
                                        threshold
                                    )
                                    
                                    if result:
                                        st.success(msg)
                                        st.json(result)
                                        st.session_state.ai_analysis_results.append(f"åŸºç¤ç•°å¸¸æª¢æ¸¬: {msg}")
                                    else:
                                        st.error(msg)
                
                with col2:
                    if "ğŸ¤– MLç•°å¸¸æª¢æ¸¬" in ai_features:
                        st.markdown("**ğŸ¤– MLç•°å¸¸æª¢æ¸¬**")
                        
                        if AI_ANOMALY_AVAILABLE:
                            if st.button("ğŸ¤– åŸ·è¡ŒMLç•°å¸¸æª¢æ¸¬", key="ml_anomaly"):
                                with st.spinner("MLç®—æ³•åˆ†æä¸­..."):
                                    result_df, msg = ai_engine.advanced_anomaly_detection(current_df)
                                    
                                    if result_df is not None:
                                        st.success(msg)
                                        
                                        # é¡¯ç¤ºç•°å¸¸é»çµ±è¨ˆ
                                        anomaly_points = result_df[result_df['is_anomaly'] == True]
                                        if len(anomaly_points) > 0:
                                            st.markdown(f"**ç™¼ç¾ {len(anomaly_points)} å€‹ç•°å¸¸é»**")
                                            
                                            # é¡¯ç¤ºç•°å¸¸åˆ†æ•¸åˆ†å¸ƒ
                                            fig, ax = plt.subplots(figsize=(8, 4))
                                            ax.hist(result_df['anomaly_score'].dropna(), bins=20, alpha=0.7)
                                            ax.set_title("ç•°å¸¸åˆ†æ•¸åˆ†å¸ƒ")
                                            ax.set_xlabel("ç•°å¸¸åˆ†æ•¸")
                                            ax.set_ylabel("é »æ¬¡")
                                            st.pyplot(fig)
                                        else:
                                            st.info("ğŸ‰ æœªç™¼ç¾æ˜é¡¯ç•°å¸¸")
                                        
                                        st.session_state.ai_analysis_results.append(f"MLç•°å¸¸æª¢æ¸¬: {msg}")
                                    else:
                                        st.error(msg)
                        else:
                            st.warning("éœ€è¦å®‰è£ scikit-learn")
                
                # è¶¨å‹¢åˆ†æ
                if "ğŸ“ˆ è¶¨å‹¢åˆ†æ" in ai_features:
                    st.markdown("#### ğŸ“ˆ è¶¨å‹¢åˆ†æ")
                    
                    trend_columns = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME', # universal_analysis_platform_v8_5_ai_ready.py
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import io
import re
from datetime import datetime
import numpy as np

# ğŸš€ é€æ­¥å¼•å…¥AIåŠŸèƒ½çš„å¥—ä»¶ (å¯é¸å®‰è£)
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    AI_ANOMALY_AVAILABLE = True
except ImportError:
    AI_ANOMALY_AVAILABLE = False

try:
    import plotly.graph_objects as go
    import plotly.express as px
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

try:
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# --- ç‰ˆæœ¬è³‡è¨Šè¨­å®š ---
VERSION = "v8.5 AI-Ready"
VERSION_DATE = "2025å¹´6æœˆ"
VERSION_FEATURES = [
    "ğŸ¨ ä¿æŒv8.0ç¾åŒ–ç•Œé¢è¨­è¨ˆ",
    "ğŸ¤– æ–°å¢AIç•°å¸¸æª¢æ¸¬åŠŸèƒ½ (å¯é¸)",
    "ğŸ“ˆ å¢å¼·è¶¨å‹¢åˆ†æèˆ‡é æ¸¬",
    "ğŸ” æ™ºèƒ½æ•¸æ“šæ´å¯Ÿå»ºè­°",
    "ğŸ“Š äº¤äº’å¼åœ–è¡¨æ”¯æ´ (Plotly)",
    "ğŸ’¬ åŸºç¤æ™ºèƒ½å°è©±åŠŸèƒ½",
    "ğŸ“ æ™ºèƒ½å ±å‘Šç”Ÿæˆå™¨",
    "ğŸ¯ æ¼¸é€²å¼AIåŠŸèƒ½éƒ¨ç½²"
]

# --- AIåŠŸèƒ½æ¨¡çµ„ (è¼•é‡ç‰ˆ) ---
class BasicAIEngine:
    """åŸºç¤AIå¼•æ“ - ä¸ä¾è³´è¤‡é›œå¥—ä»¶"""
    
    def __init__(self):
        self.analysis_history = []
    
    def basic_anomaly_detection(self, data_series, threshold_factor=2.5):
        """åŸºç¤ç•°å¸¸æª¢æ¸¬ - ä½¿ç”¨çµ±è¨ˆæ–¹æ³•"""
        if len(data_series) < 10:
            return None, "æ•¸æ“šé‡ä¸è¶³"
        
        try:
            clean_data = pd.to_numeric(data_series, errors='coerce').dropna()
            if len(clean_data) < 5:
                return None, "æœ‰æ•ˆæ•¸æ“šé»å¤ªå°‘"
            
            mean = clean_data.mean()
            std = clean_data.std()
            
            # ä½¿ç”¨çµ±è¨ˆæ–¹æ³•æª¢æ¸¬ç•°å¸¸
            upper_bound = mean + threshold_factor * std
            lower_bound = mean - threshold_factor * std
            
            anomalies = clean_data[(clean_data > upper_bound) | (clean_data < lower_bound)]
            
            result = {
                'total_points': len(clean_data),
                'anomaly_count': len(anomalies),
                'anomaly_indices': anomalies.index.tolist(),
                'bounds': (lower_bound, upper_bound),
                'statistics': {'mean': mean, 'std': std}
            }
            
            return result, f"ç™¼ç¾ {len(anomalies)} å€‹ç•°å¸¸é»"
            
        except Exception as e:
            return None, f"ç•°å¸¸æª¢æ¸¬å¤±æ•—: {str(e)}"
    
    def advanced_anomaly_detection(self, df, columns=None):
        """é€²éšç•°å¸¸æª¢æ¸¬ - ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’ (éœ€è¦scikit-learn)"""
        if not AI_ANOMALY_AVAILABLE:
            return None, "éœ€è¦å®‰è£ scikit-learn å¥—ä»¶"
        
        try:
            if columns is None:
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                columns = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME']]
            
            if not columns:
                return None, "ç„¡å¯ç”¨çš„æ•¸å€¼æ¬„ä½"
            
            data = df[columns].dropna()
            if len(data) < 10:
                return None, "æ•¸æ“šé‡ä¸è¶³é€²è¡ŒMLç•°å¸¸æª¢æ¸¬"
            
            # ä½¿ç”¨Isolation Forest
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(data)
            
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(scaled_data)
            anomaly_scores = iso_forest.score_samples(scaled_data)
            
            result_df = df.copy()
            result_df['is_anomaly'] = False
            result_df.loc[data.index, 'is_anomaly'] = anomaly_labels == -1
            result_df.loc[data.index, 'anomaly_score'] = anomaly_scores
            
            anomaly_count = sum(anomaly_labels == -1)
            
            return result_df, f"MLæª¢æ¸¬å®Œæˆï¼šç™¼ç¾ {anomaly_count} å€‹ç•°å¸¸é»"
            
        except Exception as e:
            return None, f"MLç•°å¸¸æª¢æ¸¬å¤±æ•—: {str(e)}"
    
    def trend_analysis(self, data_series, window_size=20):
        """è¶¨å‹¢åˆ†æ"""
        try:
            clean_data = pd.to_numeric(data_series, errors='coerce').dropna()
            if len(clean_data) < window_size:
                return None, f"æ•¸æ“šé‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘{window_size}å€‹é»ï¼‰"
            
            # è¨ˆç®—ç§»å‹•å¹³å‡
            rolling_mean = clean_data.rolling(window=window_size).mean()
            
            # ç°¡å–®ç·šæ€§è¶¨å‹¢
            x = np.arange(len(clean_data))
            if SCIPY_AVAILABLE:
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, clean_data)
                trend_strength = abs(r_value)
            else:
                # ç°¡å–®ç·šæ€§å›æ­¸
                slope = np.polyfit(x, clean_data, 1)[0]
                trend_strength = 0.5  # é è¨­å€¼
                r_value = 0.5
            
            # åˆ¤æ–·è¶¨å‹¢æ–¹å‘
            if slope > 0.01:
                trend_direction = "ä¸Šå‡"
            elif slope < -0.01:
                trend_direction = "ä¸‹é™"
            else:
                trend_direction = "ç©©å®š"
            
            result = {
                'slope': slope,
                'trend_direction': trend_direction,
                'trend_strength': trend_strength,
                'rolling_mean': rolling_mean,
                'analysis': f"è¶¨å‹¢æ–¹å‘ï¼š{trend_direction}ï¼Œå¼·åº¦ï¼š{'å¼·' if trend_strength > 0.7 else 'ä¸­' if trend_strength > 0.3 else 'å¼±'}"
            }
            
            return result, "è¶¨å‹¢åˆ†æå®Œæˆ"
            
        except Exception as e:
            return None, f"è¶¨å‹¢åˆ†æå¤±æ•—: {str(e)}"
    
    def generate_insights(self, df, analysis_results=None):
        """ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ"""
        insights = []
        
        if df is None or df.empty:
            return ["æ•¸æ“šç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆæ´å¯Ÿ"]
        
        # åŸºæœ¬çµ±è¨ˆæ´å¯Ÿ
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            insights.append(f"ğŸ“Š æ•¸æ“šåŒ…å« {len(df)} ç­†è¨˜éŒ„å’Œ {len(numeric_cols)} å€‹æ•¸å€¼æ¬„ä½")
            
            # æª¢æŸ¥æ•¸æ“šè®ŠåŒ–ç¨‹åº¦
            for col in numeric_cols[:3]:  # åªåˆ†æå‰3å€‹æ¬„ä½
                data = pd.to_numeric(df[col], errors='coerce').dropna()
                if len(data) > 1:
                    cv = data.std() / data.mean() if data.mean() != 0 else 0
                    if cv > 0.5:
                        insights.append(f"ğŸ” {col} è®ŠåŒ–ç¨‹åº¦è¼ƒå¤§ (è®Šç•°ä¿‚æ•¸: {cv:.2f})")
                    elif cv < 0.1:
                        insights.append(f"ğŸ“ˆ {col} ç›¸å°ç©©å®š (è®Šç•°ä¿‚æ•¸: {cv:.2f})")
        
        # åŸºæ–¼åˆ†æçµæœçš„æ´å¯Ÿ
        if analysis_results:
            for result in analysis_results:
                if "ç•°å¸¸" in result:
                    insights.append("âš ï¸ å»ºè­°é‡é»é—œæ³¨æª¢æ¸¬åˆ°çš„ç•°å¸¸é»")
                if "è¶¨å‹¢" in result:
                    insights.append("ğŸ“ˆ å»ºè­°æŒçºŒç›£æ§è¶¨å‹¢è®ŠåŒ–")
        
        # é€šç”¨å»ºè­°
        insights.append("ğŸ’¡ å»ºè­°å®šæœŸåŸ·è¡Œç•°å¸¸æª¢æ¸¬ä»¥ç¶­è­·æ•¸æ“šå“è³ª")
        insights.append("ğŸ¯ å¯å˜—è©¦èª¿æ•´æ™‚é–“ç¯„åœä¾†èšç„¦ç‰¹å®šæ™‚æ®µçš„åˆ†æ")
        
        return insights

# --- æ™ºèƒ½å°è©±ç³»çµ± (ç°¡åŒ–ç‰ˆ) ---
class SimpleConversationSystem:
    def __init__(self):
        self.responses = {
            'greeting': ['æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æ•¸æ“šåˆ†æåŠ©æ‰‹ï¼Œæœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨çš„å—ï¼Ÿ'],
            'analysis': ['æˆ‘æ­£åœ¨åˆ†ææ‚¨çš„æ•¸æ“š...', 'è®“æˆ‘æª¢æŸ¥ä¸€ä¸‹æ•¸æ“šæ¨¡å¼...'],
            'anomaly': ['æˆ‘ç™¼ç¾äº†ä¸€äº›ç•°å¸¸é»ï¼Œå»ºè­°æ‚¨ä»”ç´°æª¢æŸ¥', 'æ•¸æ“šä¸­å­˜åœ¨ä¸€äº›ä¸å°‹å¸¸çš„æ¨¡å¼'],
            'trend': ['æ ¹æ“šæ•¸æ“šè¶¨å‹¢åˆ†æ...', 'å¾æ™‚é–“åºåˆ—ä¾†çœ‹...'],
            'help': ['æ‚¨å¯ä»¥å•æˆ‘é—œæ–¼ç•°å¸¸æª¢æ¸¬ã€è¶¨å‹¢åˆ†æã€æˆ–æ•¸æ“šçµ±è¨ˆçš„å•é¡Œ']
        }
    
    def process_query(self, query, df_info=None):
        """è™•ç†ç”¨æˆ¶æŸ¥è©¢"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['ä½ å¥½', 'hello', 'å—¨']):
            return self.responses['greeting'][0]
        elif any(word in query_lower for word in ['ç•°å¸¸', 'anomaly', 'å•é¡Œ']):
            return "å»ºè­°åŸ·è¡Œç•°å¸¸æª¢æ¸¬åŠŸèƒ½ä¾†è­˜åˆ¥æ•¸æ“šä¸­çš„ç•°å¸¸æ¨¡å¼ã€‚æ‚¨å¯ä»¥åœ¨å´é‚Šæ¬„é¸æ“‡ç•°å¸¸æª¢æ¸¬é¸é …ã€‚"
        elif any(word in query_lower for word in ['è¶¨å‹¢', 'trend', 'é æ¸¬']):
            return "æˆ‘å¯ä»¥å¹«æ‚¨åˆ†ææ•¸æ“šè¶¨å‹¢ã€‚è«‹åœ¨åˆ†æé¸é …ä¸­é¸æ“‡è¶¨å‹¢åˆ†æåŠŸèƒ½ã€‚"
        elif any(word in query_lower for word in ['æœ€é«˜', 'maximum', 'max']):
            return "è«‹æŸ¥çœ‹çµ±è¨ˆè¡¨æ ¼ä¸­çš„Tmaxæ¬„ä½ï¼Œæˆ–ä½¿ç”¨åœ–è¡¨ä¸Šçš„æ•¸æ“šé»æç¤ºåŠŸèƒ½ã€‚"
        elif any(word in query_lower for word in ['å¹«åŠ©', 'help', 'æ€éº¼ç”¨']):
            return self.responses['help'][0]
        else:
            return f"æˆ‘ç†è§£æ‚¨æƒ³äº†è§£ï¼š{query}ã€‚è«‹å˜—è©¦ä½¿ç”¨æ›´å…·é«”çš„é—œéµè©ï¼Œå¦‚'ç•°å¸¸æª¢æ¸¬'ã€'è¶¨å‹¢åˆ†æ'ã€'æœ€é«˜æº«åº¦'ç­‰ã€‚"

# --- å‰µå»ºäº¤äº’å¼åœ–è¡¨ (å¦‚æœæœ‰Plotly) ---
def create_interactive_chart(df, columns, title="Interactive Analysis"):
    """å‰µå»ºäº¤äº’å¼åœ–è¡¨"""
    if not PLOTLY_AVAILABLE:
        return None
    
    if df is None or df.empty or not columns:
        return None
    
    fig = go.Figure()
    
    for col in columns[:5]:  # é™åˆ¶æœ€å¤š5å€‹series
        if col in df.columns:
            y_data = pd.to_numeric(df[col], errors='coerce').dropna()
            if not y_data.empty:
                fig.add_trace(go.Scatter(
                    x=df.index.total_seconds(),
                    y=y_data,
                    mode='lines',
                    name=col.replace('YOKO: ', '').replace('PTAT: ', ''),
                    line=dict(width=2),
                    hovertemplate=f'{col}<br>æ™‚é–“: %{{x}}s<br>æ•¸å€¼: %{{y}}<extra></extra>'
                ))
    
    fig.update_layout(
        title=title,
        xaxis_title="æ™‚é–“ (ç§’)",
        yaxis_title="æ•¸å€¼",
        hovermode='x unified',
        showlegend=True,
        height=500,
        template='plotly_white'
    )
    
    return fig

# --- å­æ¨¡çµ„ï¼šPTAT Log è§£æå™¨ (ä¿®å¾©ç‰ˆ) ---
def parse_ptat(file_content):
    try:
        file_content.seek(0)
        df = pd.read_csv(file_content, header=0, thousands=',', low_memory=False)
        df.columns = df.columns.str.strip()
        time_column = 'Time'
        if time_column not in df.columns:
            return None, "PTAT Logä¸­æ‰¾ä¸åˆ° 'Time' æ¬„ä½"
        time_series = df[time_column].astype(str).str.strip()
        time_series_cleaned = time_series.str.replace(r':(\d{3})$', r'.\1', regex=True)
        datetime_series = pd.to_datetime(time_series_cleaned, format='%H:%M:%S.%f', errors='coerce')
        valid_times_mask = datetime_series.notna()
        df = df[valid_times_mask].copy()
        if df.empty:
            return None, "PTAT Logæ™‚é–“æ ¼å¼ç„¡æ³•è§£æ"
        valid_datetimes = datetime_series[valid_times_mask]
        df['time_index'] = valid_datetimes - valid_datetimes.iloc[0]
        df = df.add_prefix('PTAT: ')
        df.rename(columns={'PTAT: time_index': 'time_index'}, inplace=True)
        return df.set_index('time_index'), "PTAT Log"
    except Exception as e:
        return None, f"è§£æPTAT Logæ™‚å‡ºéŒ¯: {e}"

# --- å­æ¨¡çµ„ï¼šYOKOGAWA Log è§£æå™¨ (éœé»˜æ™ºèƒ½ç‰ˆ) ---
def parse_yokogawa(file_content, is_excel=False):
    try:
        read_func = pd.read_excel if is_excel else pd.read_csv
        
        if is_excel:
            possible_headers = [29, 28, 30, 27]
        else:
            possible_headers = [0, 1, 2]
            
        df = None
        found_time_col = None
        successful_header = None
        
        for header_row in possible_headers:
            try:
                file_content.seek(0)
                df = read_func(file_content, header=header_row, thousands=',')
                df.columns = df.columns.str.strip()
                
                time_candidates = ['Time', 'TIME', 'time', 'Date', 'DATE', 'date', 
                                 'DateTime', 'DATETIME', 'datetime', 'æ™‚é–“', 'æ—¥æœŸæ™‚é–“',
                                 'Timestamp', 'TIMESTAMP', 'timestamp']
                
                for candidate in time_candidates:
                    if candidate in df.columns:
                        found_time_col = candidate
                        successful_header = header_row
                        break
                
                if found_time_col:
                    break
                    
            except Exception as e:
                continue
        
        if df is None or found_time_col is None:
            error_msg = "YOKOGAWA Logä¸­æ‰¾ä¸åˆ°æ™‚é–“æ¬„ä½ã€‚"
            if df is not None:
                error_msg += f" å¯ç”¨æ¬„ä½: {list(df.columns)[:15]}"
            return None, error_msg
        
        time_column = found_time_col
        
        if is_excel and successful_header == 29:
            try:
                file_content.seek(0)
                ch_row = pd.read_excel(file_content, header=None, skiprows=27, nrows=1).iloc[0]
                file_content.seek(0)
                tag_row = pd.read_excel(file_content, header=None, skiprows=28, nrows=1).iloc[0]
                
                new_column_names = {}
                for i, original_col in enumerate(df.columns):
                    if i < len(ch_row) and i < len(tag_row):
                        ch_name = str(ch_row.iloc[i]).strip() if pd.notna(ch_row.iloc[i]) else ""
                        tag_name = str(tag_row.iloc[i]).strip() if pd.notna(tag_row.iloc[i]) else ""
                        
                        if tag_name and tag_name != 'nan' and tag_name != 'Tag':
                            new_column_names[original_col] = tag_name
                        elif ch_name and ch_name != 'nan' and ch_name.startswith('CH'):
                            new_column_names[original_col] = ch_name
                        else:
                            new_column_names[original_col] = original_col
                    else:
                        new_column_names[original_col] = original_col
                
                df.rename(columns=new_column_names, inplace=True)
                
            except Exception as e:
                pass
        
        time_series = df[time_column].astype(str).str.strip()
        
        try:
            df['time_index'] = pd.to_timedelta(time_series + ':00').fillna(pd.to_timedelta('00:00:00'))
            
            if df['time_index'].isna().all():
                raise ValueError("Timedelta è½‰æ›å¤±æ•—")
                
        except:
            try:
                datetime_series = pd.to_datetime(time_series, format='%H:%M:%S', errors='coerce')
                if datetime_series.notna().sum() == 0:
                    datetime_series = pd.to_datetime(time_series, errors='coerce')
                
                df['time_index'] = datetime_series - datetime_series.iloc[0]
                
            except Exception as e:
                return None, f"ç„¡æ³•è§£ææ™‚é–“æ ¼å¼ '{time_column}': {e}. æ¨£æœ¬: {time_series.head(3).tolist()}"
        
        valid_times_mask = df['time_index'].notna()
        if valid_times_mask.sum() == 0:
            return None, f"æ™‚é–“æ¬„ä½ '{time_column}' ä¸­æ²’æœ‰æœ‰æ•ˆçš„æ™‚é–“æ•¸æ“š"
        
        df = df[valid_times_mask].copy()
        
        if len(df) > 0:
            start_time = df['time_index'].iloc[0]
            df['time_index'] = df['time_index'] - start_time
        
        numeric_columns = df.select_dtypes(include=['number']).columns
        for col in numeric_columns:
            if col != 'time_index':
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df = df.add_prefix('YOKO: ')
        df.rename(columns={'YOKO: time_index': 'time_index'}, inplace=True)
        
        return df.set_index('time_index'), "YOKOGAWA Log"
        
    except Exception as e:
        return None, f"è§£æYOKOGAWA Logæ™‚å‡ºéŒ¯: {e}"

# --- ä¸»æ¨¡çµ„ï¼šè§£æå™¨èª¿åº¦ä¸­å¿ƒ (éœé»˜ç‰ˆ) ---
def parse_dispatcher(uploaded_file):
    filename = uploaded_file.name
    file_content = io.BytesIO(uploaded_file.getvalue())
    is_excel = '.xlsx' in filename.lower() or '.xls' in filename.lower()
    
    try:
        if is_excel:
            try:
                file_content.seek(0)
                df_sniff = pd.read_excel(file_content, header=None, skiprows=27, nrows=1)
                
                ch_found = False
                for _, row in df_sniff.iterrows():
                    for cell in row:
                        if isinstance(cell, str) and re.match(r'CH\d{3}', cell.strip()):
                            ch_found = True
                            break
                    if ch_found:
                        break
                
                file_content.seek(0)
                return parse_yokogawa(file_content, is_excel)
                    
            except Exception as e:
                file_content.seek(0)
                return parse_yokogawa(file_content, is_excel)
        
        else:
            try:
                file_content.seek(0)
                first_lines_text = "".join([file_content.readline().decode('utf-8', errors='ignore') for _ in range(10)])
                file_content.seek(0)
                
                if 'MSR Package Temperature' in first_lines_text:
                    return parse_ptat(file_content)
                else:
                    pass
                    
            except Exception as e:
                pass
        
    except Exception as e:
        pass
        
    return None, f"æœªçŸ¥çš„Logæª”æ¡ˆæ ¼å¼: {filename}"

# --- æº«åº¦çµ±è¨ˆè¨ˆç®—å‡½å¼ ---
def calculate_temp_stats(df, x_limits=None):
    """è¨ˆç®—æº«åº¦çµ±è¨ˆæ•¸æ“šï¼ˆæœ€å¤§å€¼å’Œå¹³å‡å€¼ï¼‰"""
    if df is None or df.empty:
        return pd.DataFrame()
    
    df_stats = df.copy()
    if x_limits:
        x_min_td = pd.to_timedelta(x_limits[0], unit='s')
        x_max_td = pd.to_timedelta(x_limits[1], unit='s')
        df_stats = df_stats[(df_stats.index >= x_min_td) & (df_stats.index <= x_max_td)]
    
    if df_stats.empty:
        return pd.DataFrame()
    
    numeric_cols = df_stats.select_dtypes(include=['number']).columns
    temp_cols = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME']]
    
    stats_data = []
    for col in temp_cols:
        y_data = pd.to_numeric(df_stats[col], errors='coerce')
        if not y_data.isna().all():
            t_max = y_data.max()
            t_avg = y_data.mean()
            stats_data.append({
                'é€šé“åç¨±': col,
                'Tmax (Â°C)': f"{t_max:.2f}" if pd.notna(t_max) else "N/A",
                'Tavg (Â°C)': f"{t_avg:.2f}" if pd.notna(t_avg) else "N/A"
            })
    
    return pd.DataFrame(stats_data)

# --- PTAT Log å°ˆç”¨çµ±è¨ˆè¨ˆç®—å‡½å¼ ---
def calculate_ptat_stats(df, x_limits=None):
    """è¨ˆç®—PTAT Logçš„å°ˆç”¨çµ±è¨ˆæ•¸æ“š"""
    if df is None or df.empty:
        return None, None, None
    
    df_stats = df.copy()
    if x_limits:
        x_min_td = pd.to_timedelta(x_limits[0], unit='s')
        x_max_td = pd.to_timedelta(x_limits[1], unit='s')
        df_stats = df_stats[(df_stats.index >= x_min_td) & (df_stats.index <= x_max_td)]
    
    if df_stats.empty:
        return None, None, None
    
    freq_stats = []
    freq_cols = [col for col in df_stats.columns if 'frequency' in col.lower() and 'core' in col.lower()]
    
    lfm_value = "N/A"
    hfm_value = "N/A"
    
    for col in df_stats.columns:
        if 'lfm' in col.lower():
            lfm_data = pd.to_numeric(df_stats[col], errors='coerce').dropna()
            if len(lfm_data) > 0:
                lfm_value = f"{lfm_data.iloc[0]:.0f} MHz"
        elif 'hfm' in col.lower():
            hfm_data = pd.to_numeric(df_stats[col], errors='coerce').dropna()
            if len(hfm_data) > 0:
                hfm_value = f"{hfm_data.iloc[0]:.0f} MHz"
    
    if lfm_value == "N/A" or hfm_value == "N/A":
        all_freq_data = []
        for col in freq_cols:
            freq_data = pd.to_numeric(df_stats[col], errors='coerce').dropna()
            all_freq_data.extend(freq_data.tolist())
        
        if all_freq_data:
            if lfm_value == "N/A":
                lfm_value = f"{min(all_freq_data):.0f} MHz (ä¼°ç®—)"
            if hfm_value == "N/A":
                hfm_value = f"{max(all_freq_data):.0f} MHz (ä¼°ç®—)"
    
    for col in freq_cols:
        freq_data = pd.to_numeric(df_stats[col], errors='coerce').dropna()
        if len(freq_data) > 0:
            freq_stats.append({
                'Core': col.replace('PTAT: ', ''),
                'Max (MHz)': f"{freq_data.max():.0f}",
                'Min (MHz)': f"{freq_data.min():.0f}",
                'Avg (MHz)': f"{freq_data.mean():.0f}"
            })
    
    if freq_stats:
        freq_stats.append({
            'Core': '--- åƒè€ƒå€¼ ---',
            'Max (MHz)': '',
            'Min (MHz)': '',
            'Avg (MHz)': ''
        })
        freq_stats.append({
            'Core': 'LFM (Low Freq Mode)',
            'Max (MHz)': lfm_value,
            'Min (MHz)': '',
            'Avg (MHz)': ''
        })
        freq_stats.append({
            'Core': 'HFM (High Freq Mode)',
            'Max (MHz)': hfm_value,
            'Min (MHz)': '',
            'Avg (MHz)': ''
        })
    
    freq_df = pd.DataFrame(freq_stats) if freq_stats else None
    
    power_stats = []
    power_cols = [col for col in df_stats.columns if 'power' in col.lower() and 'package' in col.lower()]
    
    for col in power_cols:
        power_data = pd.to_numeric(df_stats[col], errors='coerce').dropna()
        if len(power_data) > 0:
            power_stats.append({
                'Power Type': col.replace('PTAT: ', ''),
                'Max (W)': f"{power_data.max():.2f}",
                'Min (W)': f"{power_data.min():.2f}",
                'Avg (W)': f"{power_data.mean():.2f}"
            })
    
    power_df = pd.DataFrame(power_stats) if power_stats else None
    
    temp_stats = []
    temp_cols = [col for col in df_stats.columns if 'temperature' in col.lower() and 'package' in col.lower() and 'msr' in col.lower()]
    
    for col in temp_cols:
        temp_data = pd.to_numeric(df_stats[col], errors='coerce').dropna()
        if len(temp_data) > 0:
            temp_stats.append({
                'Temperature Type': col.replace('PTAT: ', ''),
                'Max (Â°C)': f"{temp_data.max():.2f}",
                'Min (Â°C)': f"{temp_data.min():.2f}",
                'Avg (Â°C)': f"{temp_data.mean():.2f}"
            })
    
    temp_df = pd.DataFrame(temp_stats) if temp_stats else None
    
    return freq_df, power_df, temp_df

# --- åœ–è¡¨ç¹ªè£½å‡½å¼ (æ”¹é€²ç‰ˆ) ---
def generate_yokogawa_temp_chart(df, x_limits=None, y_limits=None):
    """æ”¹é€²ç‰ˆYOKOGAWAæº«åº¦åœ–è¡¨ï¼Œæ”¯æ´æ™‚é–“ç¯„åœå’ŒYè»¸ç¯„åœèª¿æ•´"""
    if df is None: 
        return None
    
    df_chart = df.copy()
    if x_limits:
        x_min_td = pd.to_timedelta(x_limits[0], unit='s')
        x_max_td = pd.to_timedelta(x_limits[1], unit='s')
        df_chart = df_chart[(df_chart.index >= x_min_td) & (df_chart.index <= x_max_td)]
    
    if df_chart.empty:
        return None
    
    fig, ax = plt.subplots(figsize=(10.2, 6.8))
    
    numeric_cols = df_chart.select_dtypes(include=['number']).columns
    cols_to_plot = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME']]
    
    max_channels = 15
    if len(cols_to_plot) > max_channels:
        cols_to_plot = cols_to_plot[:max_channels]
    
    for col in cols_to_plot:
        y_data = pd.to_numeric(df_chart[col], errors='coerce')
        if not y_data.isna().all():
            ax.plot(df_chart.index.total_seconds(), y_data, label=col, linewidth=1)
        
    ax.set_title("YOKOGAWA All Channel Temperature Plot", fontsize=14, fontweight='bold')
    ax.set_xlabel("Elapsed Time (seconds)", fontsize=11)
    ax.set_ylabel("Temperature (Â°C)", fontsize=11)
    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax.legend(title="Channels", bbox_to_anchor=(1.04, 1), loc="upper left", fontsize=7)
    
    if x_limits:
        ax.set_xlim(x_limits)
    
    if y_limits:
        ax.set_ylim(y_limits)
    
    fig.tight_layout()
    return fig

def generate
