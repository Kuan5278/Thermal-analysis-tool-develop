# thermal_analysis_platform_v10.3.8_optimized_fixed_with_burnin_plus_textfile.py
# æº«åº¦æ•¸æ“šè¦–è¦ºåŒ–å¹³å° - v10.3.8 å¤šæª”æ¡ˆç¨ç«‹åˆ†æ + Summaryæ•´åˆç‰ˆ + Burn-in Log æ”¯æ´ + æ–‡å­—æª”æ”¯æ´

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import io
import re
from datetime import datetime, date
import numpy as np
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import json
import os

# ç‰ˆæœ¬è³‡è¨Š
VERSION = "v10.3.8 Multi-File Analysis with Summary + Burn-in Log Support + Text File Support"
VERSION_DATE = "2025å¹´6æœˆ"

# =============================================================================
# 0. è¨ªå•è¨ˆæ•¸å™¨ (Visit Counter)
# =============================================================================

class VisitCounter:
    """è¨ªå•è¨ˆæ•¸å™¨"""
    
    def __init__(self, counter_file="visit_counter.json"):
        self.counter_file = counter_file
        self.data = self._load_counter()
    
    def _load_counter(self) -> dict:
        """è¼‰å…¥è¨ˆæ•¸å™¨æ•¸æ“š"""
        try:
            if os.path.exists(self.counter_file):
                with open(self.counter_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                return {
                    "total_visits": 0,
                    "daily_visits": {},
                    "first_visit": None,
                    "last_visit": None
                }
        except Exception:
            return {
                "total_visits": 0,
                "daily_visits": {},
                "first_visit": None,
                "last_visit": None
            }
    
    def _save_counter(self):
        """ä¿å­˜è¨ˆæ•¸å™¨æ•¸æ“š"""
        try:
            with open(self.counter_file, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
    
    def increment_visit(self):
        """å¢åŠ è¨ªå•è¨ˆæ•¸"""
        now = datetime.now()
        today = now.strftime("%Y-%m-%d")
        
        # æ›´æ–°ç¸½è¨ªå•æ¬¡æ•¸
        self.data["total_visits"] += 1
        
        # æ›´æ–°ä»Šæ—¥è¨ªå•æ¬¡æ•¸
        if today not in self.data["daily_visits"]:
            self.data["daily_visits"][today] = 0
        self.data["daily_visits"][today] += 1
        
        # æ›´æ–°é¦–æ¬¡è¨ªå•æ™‚é–“
        if self.data["first_visit"] is None:
            self.data["first_visit"] = now.isoformat()
        
        # æ›´æ–°æœ€å¾Œè¨ªå•æ™‚é–“
        self.data["last_visit"] = now.isoformat()
        
        # æ¸…ç†èˆŠçš„æ—¥è¨ªå•è¨˜éŒ„ï¼ˆä¿ç•™æœ€è¿‘30å¤©ï¼‰
        self._cleanup_old_records()
        
        # ä¿å­˜æ•¸æ“š
        self._save_counter()
    
    def _cleanup_old_records(self):
        """æ¸…ç†30å¤©å‰çš„æ—¥è¨ªå•è¨˜éŒ„"""
        try:
            today = date.today()
            cutoff_date = today.replace(day=today.day-30) if today.day > 30 else today.replace(month=today.month-1, day=30)
            cutoff_str = cutoff_date.strftime("%Y-%m-%d")
            
            # ç§»é™¤30å¤©å‰çš„è¨˜éŒ„
            keys_to_remove = [k for k in self.data["daily_visits"].keys() if k < cutoff_str]
            for key in keys_to_remove:
                del self.data["daily_visits"][key]
        except Exception:
            pass
    
    def get_stats(self) -> dict:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        today = date.today().strftime("%Y-%m-%d")
        yesterday = (date.today().replace(day=date.today().day-1)).strftime("%Y-%m-%d") if date.today().day > 1 else None
        
        # è¨ˆç®—æœ€è¿‘7å¤©è¨ªå•é‡
        recent_7_days = 0
        for i in range(7):
            check_date = (date.today().replace(day=date.today().day-i)).strftime("%Y-%m-%d")
            recent_7_days += self.data["daily_visits"].get(check_date, 0)
        
        return {
            "total_visits": self.data["total_visits"],
            "today_visits": self.data["daily_visits"].get(today, 0),
            "yesterday_visits": self.data["daily_visits"].get(yesterday, 0) if yesterday else 0,
            "recent_7_days": recent_7_days,
            "first_visit": self.data["first_visit"],
            "last_visit": self.data["last_visit"],
            "active_days": len(self.data["daily_visits"])
        }

def display_visit_counter():
    """é¡¯ç¤ºè¨ªå•è¨ˆæ•¸å™¨"""
    # åˆå§‹åŒ–è¨ˆæ•¸å™¨
    if 'visit_counter' not in st.session_state:
        st.session_state.visit_counter = VisitCounter()
        st.session_state.visit_counted = False
    
    # åªåœ¨ç¬¬ä¸€æ¬¡åŠ è¼‰æ™‚è¨ˆæ•¸
    if not st.session_state.visit_counted:
        st.session_state.visit_counter.increment_visit()
        st.session_state.visit_counted = True
    
    # ç²å–çµ±è¨ˆæ•¸æ“š
    stats = st.session_state.visit_counter.get_stats()
    
    # é¡¯ç¤ºè¨ˆæ•¸å™¨
    with st.sidebar:
        st.markdown("---")
        st.markdown("### ğŸ“Š ä½¿ç”¨çµ±è¨ˆ")
        
        # ä½¿ç”¨columnsä¾†ä¸¦æ’é¡¯ç¤º
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric(
                label="ğŸ’« ç¸½è¨ªå•",
                value=f"{stats['total_visits']:,}",
                help="è‡ªé¦–æ¬¡å•Ÿå‹•ä»¥ä¾†çš„ç¸½è¨ªå•æ¬¡æ•¸"
            )
            
            st.metric(
                label="ğŸ“… ä»Šæ—¥",
                value=f"{stats['today_visits']:,}",
                delta=f"+{stats['today_visits'] - stats['yesterday_visits']}" if stats['yesterday_visits'] > 0 else None,
                help="ä»Šæ—¥è¨ªå•æ¬¡æ•¸"
            )
        
        with col2:
            st.metric(
                label="ğŸ“ˆ è¿‘7å¤©",
                value=f"{stats['recent_7_days']:,}",
                help="æœ€è¿‘7å¤©ç¸½è¨ªå•æ¬¡æ•¸"
            )
            
            st.metric(
                label="ğŸ—“ï¸ æ´»èºå¤©æ•¸",
                value=f"{stats['active_days']:,}",
                help="æœ‰è¨ªå•è¨˜éŒ„çš„å¤©æ•¸"
            )
        
        # é¡¯ç¤ºè©³ç´°ä¿¡æ¯
        with st.expander("ğŸ“‹ è©³ç´°çµ±è¨ˆ", expanded=False):
            if stats['first_visit']:
                first_visit = datetime.fromisoformat(stats['first_visit'])
                st.write(f"ğŸš€ **é¦–æ¬¡ä½¿ç”¨ï¼š** {first_visit.strftime('%Y-%m-%d %H:%M')}")
            
            if stats['last_visit']:
                last_visit = datetime.fromisoformat(stats['last_visit'])
                st.write(f"â° **æœ€å¾Œä½¿ç”¨ï¼š** {last_visit.strftime('%Y-%m-%d %H:%M')}")
            
            st.write(f"ğŸ“Š **å¹³å‡æ¯æ—¥ï¼š** {stats['total_visits'] / max(stats['active_days'], 1):.1f} æ¬¡")
            
            # é¡¯ç¤ºæœ€è¿‘å¹¾å¤©çš„è¨ªå•è¶¨å‹¢
            recent_data = []
            for i in range(6, -1, -1):  # æœ€è¿‘7å¤©ï¼Œå€’åº
                check_date = date.today().replace(day=date.today().day-i) if date.today().day > i else date.today().replace(month=date.today().month-1, day=30-i+date.today().day)
                date_str = check_date.strftime("%Y-%m-%d")
                visits = st.session_state.visit_counter.data["daily_visits"].get(date_str, 0)
                recent_data.append({
                    'date': check_date.strftime("%m/%d"),
                    'visits': visits
                })
            
            if recent_data:
                st.write("ğŸ“ˆ **æœ€è¿‘7å¤©è¶¨å‹¢ï¼š**")
                trend_text = " | ".join([f"{d['date']}: {d['visits']}" for d in recent_data])
                st.code(trend_text, language=None)

# =============================================================================
# 1. æ•¸æ“šæ¨¡å‹å±¤ (Data Model Layer)
# =============================================================================

@dataclass
class LogMetadata:
    """Logæª”æ¡ˆå…ƒæ•¸æ“š"""
    filename: str
    log_type: str
    rows: int
    columns: int
    time_range: str
    file_size_kb: float

class LogData:
    """çµ±ä¸€çš„Logæ•¸æ“šæŠ½è±¡é¡"""
    def __init__(self, df: pd.DataFrame, metadata: LogMetadata):
        self.df = df
        self.metadata = metadata
        self._numeric_columns = None
    
    @property
    def numeric_columns(self) -> List[str]:
        """ç²å–æ•¸å€¼å‹æ¬„ä½"""
        if self._numeric_columns is None:
            self._numeric_columns = self.df.select_dtypes(include=['number']).columns.tolist()
        return self._numeric_columns
    
    def get_time_range(self) -> Tuple[float, float]:
        """ç²å–æ™‚é–“ç¯„åœï¼ˆç§’ï¼‰"""
        if self.df.empty:
            return (0.0, 0.0)
        return (0.0, self.df.index.total_seconds().max())
    
    def filter_by_time(self, x_limits: Tuple[float, float]):
        """æŒ‰æ™‚é–“ç¯„åœéæ¿¾æ•¸æ“š"""
        if x_limits is None:
            return self.df
        
        x_min_td = pd.to_timedelta(x_limits[0], unit='s')
        x_max_td = pd.to_timedelta(x_limits[1], unit='s')
        return self.df[(self.df.index >= x_min_td) & (self.df.index <= x_max_td)]

# =============================================================================
# 2. è§£æå™¨å±¤ (Parser Layer) - åŒ…å«æ–°çš„æ–‡å­—æª”è§£æå™¨
# =============================================================================

class ParseLogger:
    """è§£ææ—¥èªŒç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†æ‰€æœ‰è§£æè¼¸å‡º"""
    
    def __init__(self):
        self.logs = []
        self.debug_logs = []
        self.success_logs = []
        self.error_logs = []
    
    def info(self, message: str):
        """è¨˜éŒ„ä¸€èˆ¬ä¿¡æ¯"""
        self.logs.append(f"â„¹ï¸ {message}")
    
    def debug(self, message: str):
        """è¨˜éŒ„èª¿è©¦ä¿¡æ¯"""
        self.debug_logs.append(f"ğŸ” {message}")
    
    def success(self, message: str):
        """è¨˜éŒ„æˆåŠŸä¿¡æ¯"""
        self.success_logs.append(f"âœ… {message}")
    
    def error(self, message: str):
        """è¨˜éŒ„éŒ¯èª¤ä¿¡æ¯"""
        self.error_logs.append(f"âŒ {message}")
    
    def warning(self, message: str):
        """è¨˜éŒ„è­¦å‘Šä¿¡æ¯"""
        self.logs.append(f"âš ï¸ {message}")
    
    def show_summary(self, filename: str, log_type: str):
        """é¡¯ç¤ºç°¡æ½”çš„è§£ææ‘˜è¦"""
        if self.success_logs:
            st.success(f"âœ… {log_type} è§£ææˆåŠŸï¼")
        elif self.error_logs:
            st.error(f"âŒ {filename} è§£æå¤±æ•—")
            return
    
    def show_detailed_logs(self, filename: str):
        """åœ¨æ‘ºç–Šå€åŸŸå…§é¡¯ç¤ºè©³ç´°æ—¥èªŒ"""
        with st.expander(f"ğŸ” è©³ç´°è§£ææ—¥èªŒ - {filename}", expanded=False):
            if self.debug_logs:
                st.markdown("**ğŸ” èª¿è©¦ä¿¡æ¯ï¼š**")
                for log in self.debug_logs:
                    st.code(log, language=None)
            
            if self.logs:
                st.markdown("**ğŸ“‹ è§£æéç¨‹ï¼š**")
                for log in self.logs:
                    st.write(log)
            
            if self.success_logs:
                st.markdown("**âœ… æˆåŠŸä¿¡æ¯ï¼š**")
                for log in self.success_logs:
                    st.write(log)
            
            if self.error_logs:
                st.markdown("**âŒ éŒ¯èª¤ä¿¡æ¯ï¼š**")
                for log in self.error_logs:
                    st.write(log)

class LogParser(ABC):
    """è§£æå™¨æŠ½è±¡åŸºé¡"""
    
    def __init__(self):
        self.logger = ParseLogger()
    
    @abstractmethod
    def can_parse(self, file_content: io.BytesIO, filename: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦èƒ½è§£ææ­¤æª”æ¡ˆ"""
        pass
    
    @abstractmethod
    def parse(self, file_content: io.BytesIO, filename: str) -> Optional[LogData]:
        """è§£ææª”æ¡ˆ"""
        pass
    
    @property
    @abstractmethod
    def log_type(self) -> str:
        """Logé¡å‹åç¨±"""
        pass

# =============================================================================
# æ–°å¢ï¼šé€šç”¨æ–‡å­—æª”è§£æå™¨ (Universal Text File Parser)
# =============================================================================

class TextFileParser(LogParser):
    """é€šç”¨æ–‡å­—æª”è§£æå™¨ - æ”¯æ´.log, .txtç­‰æ–‡å­—æ ¼å¼"""
    
    @property
    def log_type(self) -> str:
        return "Text/Log File"
    
    def can_parse(self, file_content: io.BytesIO, filename: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæ–‡å­—æª”æ ¼å¼"""
        try:
            filename_lower = filename.lower()
            # æ”¯æ´çš„æ–‡å­—æª”å‰¯æª”å
            text_extensions = ['.log', '.txt', '.dat', '.out']
            
            # æª¢æŸ¥å‰¯æª”å
            if any(filename_lower.endswith(ext) for ext in text_extensions):
                return True
            
            # å˜—è©¦è®€å–å‰å¹¾è¡Œåˆ¤æ–·æ˜¯å¦ç‚ºæ–‡å­—æ ¼å¼
            file_content.seek(0)
            try:
                first_content = file_content.read(1000).decode('utf-8', errors='ignore')
                # å¦‚æœåŒ…å«å¸¸è¦‹çš„æ•¸æ“šé—œéµå­—ï¼Œèªç‚ºæ˜¯å¯è§£æçš„æ–‡å­—æª”
                keywords = ['time', 'temp', 'temperature', 'freq', 'core', 'cpu', 'data', 'value']
                return any(keyword in first_content.lower() for keyword in keywords)
            except:
                return False
            
        except:
            return False
    
    def parse(self, file_content: io.BytesIO, filename: str) -> Optional[LogData]:
        """è§£ææ–‡å­—æª”"""
        try:
            file_content.seek(0)
            
            self.logger.debug(f"é–‹å§‹è§£ææ–‡å­—æª”: {filename}")
            
            # å˜—è©¦ä¸åŒç·¨ç¢¼è®€å–æ–‡ä»¶
            content = self._read_with_encoding(file_content)
            if content is None:
                self.logger.error("ç„¡æ³•è®€å–æ–‡ä»¶å…§å®¹")
                return None
            
            lines = content.split('\n')
            self.logger.debug(f"æ–‡ä»¶ç¸½è¡Œæ•¸: {len(lines)}")
            
            # åˆ†ææ–‡ä»¶çµæ§‹
            file_structure = self._analyze_file_structure(lines)
            if file_structure is None:
                self.logger.error("ç„¡æ³•åˆ†ææ–‡ä»¶çµæ§‹")
                return None
            
            # æå–æ•¸æ“š
            df = self._extract_data(lines, file_structure)
            if df is None or df.empty:
                self.logger.error("ç„¡æ³•æå–æœ‰æ•ˆæ•¸æ“š")
                return None
            
            self.logger.debug(f"åŸå§‹DataFrameå½¢ç‹€: {df.shape}")
            
            # è™•ç†æ™‚é–“æ•¸æ“š
            df = self._process_time_data(df)
            if df is None:
                self.logger.error("æ™‚é–“æ•¸æ“šè™•ç†å¤±æ•—")
                return None
            
            # æ•¸å€¼è½‰æ›
            df = self._convert_numeric_columns(df)
            
            # æ·»åŠ å‰ç¶´ä¸¦è¨­ç½®ç´¢å¼•
            df = df.add_prefix('TXT: ')
            df.rename(columns={'TXT: time_index': 'time_index'}, inplace=True)
            result_df = df.set_index('time_index')
            
            # å‰µå»ºå…ƒæ•¸æ“š
            file_size_kb = len(content.encode('utf-8')) / 1024
            time_range = f"{result_df.index.min()} åˆ° {result_df.index.max()}"
            
            metadata = LogMetadata(
                filename=filename,
                log_type=self.log_type,
                rows=result_df.shape[0],
                columns=result_df.shape[1],
                time_range=time_range,
                file_size_kb=file_size_kb
            )
            
            self.logger.success(f"æ–‡å­—æª”è§£ææˆåŠŸï¼æ•¸æ“šå½¢ç‹€: {result_df.shape}")
            return LogData(result_df, metadata)
            
        except Exception as e:
            self.logger.error(f"æ–‡å­—æª”è§£æç•°å¸¸: {e}")
            return None
    
    def _read_with_encoding(self, file_content: io.BytesIO) -> Optional[str]:
        """å˜—è©¦ä¸åŒç·¨ç¢¼è®€å–æ–‡ä»¶"""
        encodings = ['utf-8', 'gbk', 'big5', 'ascii', 'latin-1']
        
        for encoding in encodings:
            try:
                file_content.seek(0)
                content = file_content.read().decode(encoding, errors='ignore')
                if content.strip():  # ç¢ºä¿æœ‰å…§å®¹
                    self.logger.debug(f"æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–æ–‡ä»¶")
                    return content
            except Exception as e:
                self.logger.debug(f"ç·¨ç¢¼ {encoding} å¤±æ•—: {e}")
                continue
        
        return None
    
    def _analyze_file_structure(self, lines: List[str]) -> Optional[Dict]:
        """åˆ†ææ–‡ä»¶çµæ§‹"""
        structure = {
            'header_line_idx': None,
            'delimiter': ',',
            'data_start_idx': None,
            'has_header': False
        }
        
        # æ¸…ç†ç©ºè¡Œ
        non_empty_lines = [(i, line) for i, line in enumerate(lines) if line.strip()]
        if not non_empty_lines:
            return None
        
        self.logger.debug(f"éç©ºè¡Œæ•¸: {len(non_empty_lines)}")
        
        # å°‹æ‰¾å¯èƒ½çš„æ¨™é¡Œè¡Œ
        for i, (line_idx, line) in enumerate(non_empty_lines[:20]):  # æª¢æŸ¥å‰20å€‹éç©ºè¡Œ
            line_clean = line.strip().lower()
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¨™é¡Œé—œéµå­—
            title_keywords = ['time', 'date', 'temp', 'temperature', 'freq', 'frequency', 
                            'core', 'cpu', 'gpu', 'value', 'data', 'channel', 'ch']
            
            if any(keyword in line_clean for keyword in title_keywords):
                structure['header_line_idx'] = line_idx
                structure['has_header'] = True
                structure['data_start_idx'] = line_idx + 1
                
                # åˆ¤æ–·åˆ†éš”ç¬¦
                if line.count(',') >= 2:
                    structure['delimiter'] = ','
                elif line.count('\t') >= 2:
                    structure['delimiter'] = '\t'
                elif line.count(';') >= 2:
                    structure['delimiter'] = ';'
                elif line.count('|') >= 2:
                    structure['delimiter'] = '|'
                else:
                    structure['delimiter'] = None  # ç©ºæ ¼åˆ†éš”
                
                self.logger.debug(f"æ‰¾åˆ°æ¨™é¡Œè¡Œåœ¨ç¬¬ {line_idx+1} è¡Œ")
                self.logger.debug(f"åˆ†éš”ç¬¦: {structure['delimiter'] or 'ç©ºæ ¼'}")
                break
        
        # å¦‚æœæ²’æ‰¾åˆ°æ¨™é¡Œè¡Œï¼Œä½¿ç”¨ç¬¬ä¸€å€‹éç©ºè¡Œ
        if structure['header_line_idx'] is None:
            first_line_idx, first_line = non_empty_lines[0]
            structure['header_line_idx'] = first_line_idx
            structure['data_start_idx'] = first_line_idx + 1
            structure['has_header'] = False
            
            # çŒœæ¸¬åˆ†éš”ç¬¦
            if first_line.count(',') >= 1:
                structure['delimiter'] = ','
            elif first_line.count('\t') >= 1:
                structure['delimiter'] = '\t'
            else:
                structure['delimiter'] = None
            
            self.logger.debug("æœªæ‰¾åˆ°æ˜ç¢ºæ¨™é¡Œè¡Œï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œ")
        
        return structure
    
    def _extract_data(self, lines: List[str], structure: Dict) -> Optional[pd.DataFrame]:
        """æå–æ•¸æ“š"""
        try:
            header_line = lines[structure['header_line_idx']].strip()
            delimiter = structure['delimiter']
            
            # è§£ææ¨™é¡Œ
            if delimiter:
                headers = [h.strip() for h in header_line.split(delimiter)]
            else:
                headers = re.split(r'\s+', header_line.strip())
            
            # å¦‚æœæ²’æœ‰æ˜ç¢ºæ¨™é¡Œï¼Œç”Ÿæˆé»˜èªæ¨™é¡Œ
            if not structure['has_header']:
                headers = [f'Column_{i}' for i in range(len(headers))]
            
            self.logger.debug(f"æ¨™é¡Œæ¬„ä½: {headers}")
            
            # æå–æ•¸æ“šè¡Œ
            data_rows = []
            for i in range(structure['data_start_idx'], len(lines)):
                line = lines[i].strip()
                if not line:
                    continue
                
                # åˆ†å‰²æ•¸æ“š
                if delimiter:
                    row_data = [cell.strip() for cell in line.split(delimiter)]
                else:
                    row_data = re.split(r'\s+', line.strip())
                
                # ç¢ºä¿æ•¸æ“šé•·åº¦èˆ‡æ¨™é¡Œä¸€è‡´
                if len(row_data) >= len(headers):
                    data_rows.append(row_data[:len(headers)])
                elif len(row_data) > 0:
                    # è£œé½Šç¼ºå¤±çš„æ¬„ä½
                    while len(row_data) < len(headers):
                        row_data.append('')
                    data_rows.append(row_data)
            
            if not data_rows:
                self.logger.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ•¸æ“šè¡Œ")
                return None
            
            self.logger.debug(f"æå–åˆ° {len(data_rows)} è¡Œæ•¸æ“š")
            
            # å‰µå»ºDataFrame
            df = pd.DataFrame(data_rows, columns=headers)
            return df
            
        except Exception as e:
            self.logger.error(f"æ•¸æ“šæå–å¤±æ•—: {e}")
            return None
    
    def _process_time_data(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """è™•ç†æ™‚é–“æ•¸æ“š"""
        try:
            # å°‹æ‰¾æ™‚é–“æ¬„ä½
            time_candidates = ['Time', 'TIME', 'time', 'Date', 'DATE', 'date', 
                             'DateTime', 'DATETIME', 'datetime', 'Timestamp', 
                             'TIMESTAMP', 'timestamp', 'æ™‚é–“', 'æ—¥æœŸæ™‚é–“', 'Elapsed']
            
            time_col = None
            for candidate in time_candidates:
                if candidate in df.columns:
                    time_col = candidate
                    break
            
            # å¦‚æœæ²’æ‰¾åˆ°ï¼Œæª¢æŸ¥ç¬¬ä¸€æ¬„æ˜¯å¦å¯èƒ½æ˜¯æ™‚é–“
            if time_col is None and len(df.columns) > 0:
                first_col = df.columns[0]
                first_col_lower = first_col.lower()
                if any(keyword in first_col_lower for keyword in ['time', 'sec', 'min', 'hour', 'elapsed']):
                    time_col = first_col
            
            if time_col is None:
                # å‰µå»ºé»˜èªæ™‚é–“åºåˆ—
                df['time_index'] = pd.to_timedelta(range(len(df)), unit='s')
                self.logger.debug("å‰µå»ºé»˜èªæ™‚é–“åºåˆ—")
                return df
            
            self.logger.debug(f"ä½¿ç”¨æ™‚é–“æ¬„ä½: {time_col}")
            
            # å˜—è©¦è§£ææ™‚é–“
            time_series = df[time_col].astype(str).str.strip()
            
            # æ–¹æ³•1: æ•¸å€¼ç§’æ•¸
            try:
                numeric_time = pd.to_numeric(time_series, errors='coerce')
                if not numeric_time.isna().all() and (numeric_time >= 0).all():
                    df['time_index'] = pd.to_timedelta(numeric_time, unit='s')
                    self.logger.debug("æ™‚é–“è§£ææˆåŠŸ (æ•¸å€¼ç§’)")
                    return df
            except:
                pass
            
            # æ–¹æ³•2: æ™‚é–“æ ¼å¼ HH:MM:SS
            try:
                # è™•ç†å¯èƒ½çš„æ¯«ç§’
                time_series_cleaned = time_series.str.replace(r':(\d{3})$', r'.\1', regex=True)
                timedelta_series = pd.to_timedelta(time_series_cleaned, errors='coerce')
                if timedelta_series.notna().sum() > len(df) * 0.5:  # è‡³å°‘50%æˆåŠŸè§£æ
                    df['time_index'] = timedelta_series
                    self.logger.debug("æ™‚é–“è§£ææˆåŠŸ (Timedeltaæ ¼å¼)")
                    return df
            except:
                pass
            
            # æ–¹æ³•3: DateTimeæ ¼å¼
            try:
                datetime_series = pd.to_datetime(time_series, errors='coerce')
                if datetime_series.notna().sum() > len(df) * 0.5:
                    df['time_index'] = datetime_series - datetime_series.iloc[0]
                    self.logger.debug("æ™‚é–“è§£ææˆåŠŸ (DateTimeæ ¼å¼)")
                    return df
            except:
                pass
            
            # é»˜èªï¼šå‰µå»ºæ™‚é–“åºåˆ—
            df['time_index'] = pd.to_timedelta(range(len(df)), unit='s')
            self.logger.warning("ä½¿ç”¨é»˜èªæ™‚é–“åºåˆ—")
            return df
            
        except Exception as e:
            self.logger.error(f"æ™‚é–“è™•ç†ç•°å¸¸: {e}")
            df['time_index'] = pd.to_timedelta(range(len(df)), unit='s')
            return df
    
    def _convert_numeric_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """è½‰æ›æ•¸å€¼å‹æ¬„ä½"""
        try:
            numeric_count = 0
            
            for col in df.columns:
                if col in ['time_index']:
                    continue
                
                # è½‰æ›æ•¸å€¼å‹æ¬„ä½
                try:
                    # æ¸…ç†å¸¸è¦‹çš„éæ•¸å€¼å­—ç¬¦
                    df[col] = df[col].astype(str).str.replace('[^\d\.\-\+eE]', '', regex=True)
                    df[col] = df[col].replace(['', 'nan', 'NaN', 'N/A', 'n/a'], np.nan)
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    if not df[col].isna().all():
                        numeric_count += 1
                except Exception as e:
                    self.logger.debug(f"æ¬„ä½ {col} è½‰æ›å¤±æ•—: {e}")
                    pass
            
            self.logger.debug(f"è½‰æ›äº† {numeric_count} å€‹æ•¸å€¼æ¬„ä½")
            return df
            
        except Exception as e:
            self.logger.warning(f"æ•¸å€¼è½‰æ›ç•°å¸¸: {e}")
            return df

# =============================================================================
# å…¶ä»–åŸæœ‰è§£æå™¨ (ä¿æŒä¸è®Šï¼Œä½†ç‚ºäº†ç¯€çœç©ºé–“é€™è£¡çœç•¥)
# å¯¦éš›ä½¿ç”¨æ™‚éœ€è¦åŒ…å«å®Œæ•´çš„ BurnInParser, GPUMonParser, PTATParser, YokogawaParser
# =============================================================================

class BurnInParser(LogParser):
    """Burn-in Logè§£æå™¨ - ç°¡åŒ–ç‰ˆ"""
    
    @property
    def log_type(self) -> str:
        return "Burn-in Log"
    
    def can_parse(self, file_content: io.BytesIO, filename: str) -> bool:
        try:
            filename_lower = filename.lower()
            filename_indicators = any(keyword in filename_lower for keyword in [
                'burn', 'burnin', 'burn-in', 'burn_in', 'stress', 'stability'
            ])
            
            if filename_indicators:
                return True
                
            file_content.seek(0)
            first_content = ""
            for _ in range(50):
                try:
                    line = file_content.readline().decode('utf-8', errors='ignore')
                    if not line:
                        break
                    first_content += line.lower()
                except:
                    break
            
            content_indicators = [
                'core' in first_content and 'temp' in first_content,
                'core' in first_content and 'freq' in first_content,
                'burn' in first_content,
                'stress' in first_content,
            ]
            
            return any(content_indicators)
        except:
            return False
    
    def parse(self, file_content: io.BytesIO, filename: str) -> Optional[LogData]:
        # ç°¡åŒ–çš„Burn-inè§£æé‚è¼¯
        try:
            file_content.seek(0)
            df = pd.read_csv(file_content, header=0)
            
            # è™•ç†æ™‚é–“å’Œæ•¸å€¼è½‰æ›
            df['time_index'] = pd.to_timedelta(range(len(df)), unit='s')
            
            # æ·»åŠ å‰ç¶´
            df = df.add_prefix('BURNIN: ')
            df.rename(columns={'BURNIN: time_index': 'time_index'}, inplace=True)
            result_df = df.set_index('time_index')
            
            metadata = LogMetadata(
                filename=filename,
                log_type=self.log_type,
                rows=result_df.shape[0],
                columns=result_df.shape[1],
                time_range=f"{result_df.index.min()} åˆ° {result_df.index.max()}",
                file_size_kb=len(file_content.getvalue()) / 1024
            )
            
            self.logger.success(f"Burn-inè§£ææˆåŠŸï¼")
            return LogData(result_df, metadata)
        except Exception as e:
            self.logger.error(f"Burn-inè§£æå¤±æ•—: {e}")
            return None

class GPUMonParser(LogParser):
    """GPUMonè§£æå™¨ - ç°¡åŒ–ç‰ˆ"""
    
    @property
    def log_type(self) -> str:
        return "GPUMon Log"
    
    def can_parse(self, file_content: io.BytesIO, filename: str) -> bool:
        try:
            file_content.seek(0)
            first_content = file_content.read(2000).decode('utf-8', errors='ignore')
            indicators = [
                'GPU Informations' in first_content,
                'Temperature GPU (C)' in first_content,
                'gpu' in first_content.lower() and 'temp' in first_content.lower()
            ]
            return any(indicators)
        except:
            return False
    
    def parse(self, file_content: io.BytesIO, filename: str) -> Optional[LogData]:
        try:
            file_content.seek(0)
            df = pd.read_csv(file_content, header=0)
            
            df['time_index'] = pd.to_timedelta(range(len(df)), unit='s')
            df = df.add_prefix('GPU: ')
            df.rename(columns={'GPU: time_index': 'time_index'}, inplace=True)
            result_df = df.set_index('time_index')
            
            metadata = LogMetadata(
                filename=filename,
                log_type=self.log_type,
                rows=result_df.shape[0],
                columns=result_df.shape[1],
                time_range=f"{result_df.index.min()} åˆ° {result_df.index.max()}",
                file_size_kb=len(file_content.getvalue()) / 1024
            )
            
            self.logger.success(f"GPUMonè§£ææˆåŠŸï¼")
            return LogData(result_df, metadata)
        except Exception as e:
            self.logger.error(f"GPUMonè§£æå¤±æ•—: {e}")
            return None

class PTATParser(LogParser):
    """PTATè§£æå™¨ - ç°¡åŒ–ç‰ˆ"""
    
    @property
    def log_type(self) -> str:
        return "PTAT Log"
    
    def can_parse(self, file_content: io.BytesIO, filename: str) -> bool:
        try:
            file_content.seek(0)
            first_content = file_content.read(2000).decode('utf-8', errors='ignore')
            return 'MSR Package Temperature' in first_content or 'Version,Date,Time' in first_content
        except:
            return False
    
    def parse(self, file_content: io.BytesIO, filename: str) -> Optional[LogData]:
        try:
            file_content.seek(0)
            df = pd.read_csv(file_content, header=0)
            
            df['time_index'] = pd.to_timedelta(range(len(df)), unit='s')
            df = df.add_prefix('PTAT: ')
            df.rename(columns={'PTAT: time_index': 'time_index'}, inplace=True)
            result_df = df.set_index('time_index')
            
            metadata = LogMetadata(
                filename=filename,
                log_type=self.log_type,
                rows=result_df.shape[0],
                columns=result_df.shape[1],
                time_range=f"{result_df.index.min()} åˆ° {result_df.index.max()}",
                file_size_kb=len(file_content.getvalue()) / 1024
            )
            
            self.logger.success(f"PTATè§£ææˆåŠŸï¼")
            return LogData(result_df, metadata)
        except Exception as e:
            self.logger.error(f"PTATè§£æå¤±æ•—: {e}")
            return None

class YokogawaParser(LogParser):
    """YOKOGAWAè§£æå™¨ - ä½œç‚ºå…œåº•è§£æå™¨"""
    
    @property
    def log_type(self) -> str:
        return "YOKOGAWA Log"
    
    def can_parse(self, file_content: io.BytesIO, filename: str) -> bool:
        return True  # ä½œç‚ºå…œåº•è§£æå™¨
    
    def parse(self, file_content: io.BytesIO, filename: str) -> Optional[LogData]:
        try:
            file_content.seek(0)
            is_excel = '.xlsx' in filename.lower() or '.xls' in filename.lower()
            
            if is_excel:
                df = pd.read_excel(file_content, header=0)
            else:
                df = pd.read_csv(file_content, header=0)
            
            df['time_index'] = pd.to_timedelta(range(len(df)), unit='s')
            df = df.add_prefix('YOKO: ')
            df.rename(columns={'YOKO: time_index': 'time_index'}, inplace=True)
            result_df = df.set_index('time_index')
            
            metadata = LogMetadata(
                filename=filename,
                log_type=self.log_type,
                rows=result_df.shape[0],
                columns=result_df.shape[1],
                time_range=f"{result_df.index.min()} åˆ° {result_df.index.max()}",
                file_size_kb=len(file_content.getvalue()) / 1024
            )
            
            self.logger.success(f"YOKOGAWAè§£ææˆåŠŸï¼")
            return LogData(result_df, metadata)
        except Exception as e:
            self.logger.error(f"YOKOGAWAè§£æå¤±æ•—: {e}")
            return None

# =============================================================================
# è§£æå™¨è¨»å†Šç³»çµ±
# =============================================================================

class ParserRegistry:
    """è§£æå™¨è¨»å†Šç³»çµ±"""
    
    def __init__(self):
        self.parsers: List[LogParser] = []
    
    def register(self, parser: LogParser):
        """è¨»å†Šè§£æå™¨"""
        self.parsers.append(parser)
    
    def parse_file(self, uploaded_file) -> Optional[LogData]:
        """è§£ææª”æ¡ˆï¼Œè‡ªå‹•é¸æ“‡åˆé©çš„è§£æå™¨"""
        filename = uploaded_file.name
        file_content = io.BytesIO(uploaded_file.getvalue())
        
        for parser in self.parsers:
            try:
                file_content.seek(0)
                if parser.can_parse(file_content, filename):
                    file_content.seek(0)
                    result = parser.parse(file_content, filename)
                    if result is not None:
                        parser.logger.show_summary(filename, parser.log_type)
                        parser.logger.show_detailed_logs(filename)
                        return result
            except Exception as e:
                continue
        
        st.error(f"âŒ ç„¡æ³•è§£ææª”æ¡ˆ {filename}")
        return None

# =============================================================================
# çµ±è¨ˆè¨ˆç®—å±¤
# =============================================================================

class StatisticsCalculator:
    """çµ±è¨ˆè¨ˆç®—å™¨"""
    
    @staticmethod
    def calculate_temp_stats(log_data: LogData, x_limits=None):
        """è¨ˆç®—æº«åº¦çµ±è¨ˆæ•¸æ“š"""
        df = log_data.filter_by_time(x_limits)
        if df.empty:
            return pd.DataFrame()
        
        numeric_cols = df.select_dtypes(include=['number']).columns
        temp_cols = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME']]
        
        stats_data = []
        for col in temp_cols:
            y_data = pd.to_numeric(df[col], errors='coerce')
            if not y_data.isna().all():
                t_max = y_data.max()
                t_avg = y_data.mean()
                
                display_name = col
                for prefix in ['YOKO: ', 'PTAT: ', 'GPU: ', 'BURNIN: ', 'TXT: ']:
                    display_name = display_name.replace(prefix, '')
                
                if display_name.lower() in ['sec', 'time', 'rt', 'date']:
                    continue
                
                stats_data.append({
                    'é€šé“åç¨±': display_name,
                    'Tmax (Â°C)': f"{t_max:.2f}" if pd.notna(t_max) else "N/A",
                    'Tavg (Â°C)': f"{t_avg:.2f}" if pd.notna(t_avg) else "N/A"
                })
        
        return pd.DataFrame(stats_data)

# =============================================================================
# åœ–è¡¨ç”Ÿæˆå±¤
# =============================================================================

class ChartGenerator:
    """åœ–è¡¨ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_flexible_chart(log_data: LogData, left_col: str, right_col: str, x_limits, left_y_limits=None, right_y_limits=None):
        """ç”Ÿæˆéˆæ´»çš„é›™è»¸åœ–è¡¨"""
        df = log_data.filter_by_time(x_limits)
        
        if df.empty or not left_col or left_col not in df.columns:
            return None
        if right_col and right_col != 'None' and right_col not in df.columns:
            return None
        
        df_chart = df.copy()
        df_chart.loc[:, 'left_val'] = pd.to_numeric(df_chart[left_col], errors='coerce')
        if right_col and right_col != 'None':
            df_chart.loc[:, 'right_val'] = pd.to_numeric(df_chart[right_col], errors='coerce')
        
        fig, ax1 = plt.subplots(figsize=(10.2, 5.1))
        plt.title(f'{left_col} {"& " + right_col if right_col and right_col != "None" else ""}', fontsize=14, fontweight='bold')
        
        x_axis_seconds = df_chart.index.total_seconds()
        color = 'tab:blue'
        ax1.set_xlabel('Elapsed Time (seconds)', fontsize=11)
        ax1.set_ylabel(left_col, color=color, fontsize=11)
        ax1.plot(x_axis_seconds, df_chart['left_val'], color=color, linewidth=1.5)
        ax1.tick_params(axis='y', labelcolor=color)
        ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
        
        if left_y_limits:
            ax1.set_ylim(left_y_limits)
        
        if right_col and right_col != 'None':
            ax2 = ax1.twinx()
            color = 'tab:red'
            ax2.set_ylabel(right_col, color=color, fontsize=11)
            ax2.plot(x_axis_seconds, df_chart['right_val'], color=color, linewidth=1.5)
            ax2.tick_params(axis='y', labelcolor=color)
            
            if right_y_limits:
                ax2.set_ylim(right_y_limits)
        
        if x_limits:
            ax1.set_xlim(x_limits)
        
        fig.tight_layout()
        return fig

# =============================================================================
# Summaryæ•´åˆè¡¨æ ¼ç”Ÿæˆå™¨
# =============================================================================

class TemperatureSummaryGenerator:
    """æº«åº¦æ•´åˆæ‘˜è¦ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_summary_table(log_data_list: List[LogData]) -> pd.DataFrame:
        """ç”Ÿæˆæº«åº¦æ‘˜è¦è¡¨æ ¼"""
        summary_data = []
        ch_number = 1
        
        for log_data in log_data_list:
            df = log_data.df
            log_type = log_data.metadata.log_type
            filename = log_data.metadata.filename
            
            numeric_cols = df.select_dtypes(include=['number']).columns
            temp_cols = [col for col in numeric_cols if col not in ['Date', 'sec', 'RT', 'TIME']]
            
            for col in temp_cols:
                temp_data = pd.to_numeric(df[col], errors='coerce').dropna()
                if len(temp_data) > 0:
                    max_temp = temp_data.max()
                    
                    clean_col_name = col
                    for prefix in ['YOKO: ', 'PTAT: ', 'GPU: ', 'BURNIN: ', 'TXT: ']:
                        clean_col_name = clean_col_name.replace(prefix, '')
                    
                    if clean_col_name.lower() in ['sec', 'time', 'rt', 'date', 'iteration']:
                        continue
                    
                    description = ""
                    if "Text/Log" in log_type:
                        description = "Text File Data"
                    elif "GPU" in log_type:
                        description = "GPU Temperature" if "Temperature" in clean_col_name else "GPU Data"
                    elif "PTAT" in log_type:
                        description = "CPU Data"
                    elif "Burn-in" in log_type:
                        description = "Burn-in Data"
                    
                    formatted_temp = f"{max_temp:.1f}" if max_temp <= 200 else f"{max_temp/1000:.1f}"
                    
                    summary_data.append({
                        'Ch.': ch_number,
                        'Location': clean_col_name,
                        'Description': description,
                        'Spec location': "",
                        'spec': "",
                        'Ref Tc spec': "",
                        'Result (Case Temp)': formatted_temp,
                        'Source File': filename,
                        'Log Type': log_type
                    })
                    
                    ch_number += 1
        
        return pd.DataFrame(summary_data)
    
    @staticmethod
    def format_summary_table_for_display(summary_df: pd.DataFrame) -> pd.DataFrame:
        """æ ¼å¼åŒ–è¡¨æ ¼ä»¥ç¬¦åˆé¡¯ç¤ºè¦æ±‚"""
        if summary_df.empty:
            return pd.DataFrame()
        
        display_df = summary_df[['Ch.', 'Location', 'Description', 'Spec location', 'spec', 'Ref Tc spec', 'Result (Case Temp)']].copy()
        return display_df

# =============================================================================
# æ–‡å­—æª”æ¸²æŸ“å™¨
# =============================================================================

class TextFileRenderer:
    """æ–‡å­—æª”UIæ¸²æŸ“å™¨"""
    
    def __init__(self, log_data: LogData):
        self.log_data = log_data
        self.stats_calc = StatisticsCalculator()
        self.chart_gen = ChartGenerator()
    
    def render(self, file_index=None):
        """æ¸²æŸ“å®Œæ•´UI"""
        if file_index is None:
            file_index = getattr(st.session_state, 'current_file_index', 0)
        key_prefix = f"txt_{file_index}_"
        
        st.markdown("""
        <div style="background-color: #e8f5e8; border: 1px solid #4caf50; color: #2e7d32; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
            <h4>ğŸ“„ æ–‡å­—æª”è§£æå®Œæˆï¼</h4>
            <p>å·²æˆåŠŸè§£ææ‚¨çš„Log/æ–‡å­—æª”ï¼Œæ•¸æ“šå·²æº–å‚™å¥½é€²è¡Œåˆ†æ</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.success(f"ğŸ“Š æ•¸æ“šè¼‰å…¥ï¼š{self.log_data.metadata.rows} è¡Œ Ã— {self.log_data.metadata.columns} åˆ—")
        
        # å´é‚Šæ¬„æ§åˆ¶
        st.sidebar.markdown("### âš™ï¸ æ–‡å­—æª”åœ–è¡¨è¨­å®š")
        
        numeric_columns = self.log_data.numeric_columns
        if not numeric_columns:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ•¸å€¼å‹æ¬„ä½ï¼Œç„¡æ³•ç”Ÿæˆåœ–è¡¨")
            st.markdown("### ğŸ“Š æ•¸æ“šé è¦½")
            st.dataframe(self.log_data.df.head(20), use_container_width=True)
            return
        
        chart_mode = st.sidebar.radio(
            "ğŸ“ˆ åœ–è¡¨æ¨¡å¼", 
            ["å…¨æ¬„ä½åœ–è¡¨", "è‡ªå®šç¾©é›™è»¸åœ–"], 
            key=f"{key_prefix}chart_mode"
        )
        
        time_min, time_max = self.log_data.get_time_range()
        x_range = st.sidebar.slider(
            "é¸æ“‡æ™‚é–“ç¯„åœ (ç§’)", 
            min_value=time_min, 
            max_value=time_max, 
            value=(time_min, time_max), 
            step=1.0, 
            key=f"{key_prefix}x_range"
        )
        
        if chart_mode == "å…¨æ¬„ä½åœ–è¡¨":
            # æ¬„ä½é¸æ“‡
            max_columns = min(15, len(numeric_columns))
            selected_columns = st.sidebar.multiselect(
                f"é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ (æœ€å¤š{max_columns}å€‹)",
                options=numeric_columns,
                default=numeric_columns[:max_columns],
                key=f"{key_prefix}selected_columns"
            )
            
            # Yè»¸ç¯„åœ
            y_range_enabled = st.sidebar.checkbox("å•Ÿç”¨Yè»¸ç¯„åœé™åˆ¶", key=f"{key_prefix}y_range_enabled")
            y_range = None
            if y_range_enabled:
                col1, col2 = st.sidebar.columns(2)
                with col1:
                    y_min = st.number_input("Yè»¸æœ€å°å€¼", value=0.0, key=f"{key_prefix}y_min")
                with col2:
                    y_max = st.number_input("Yè»¸æœ€å¤§å€¼", value=100.0, key=f"{key_prefix}y_max")
                y_range = (y_min, y_max)
            
            # ç”Ÿæˆåœ–è¡¨
            st.markdown("### ğŸ“Š æ–‡å­—æª”æ•¸æ“šåœ–è¡¨")
            if selected_columns:
                chart = self._generate_multi_line_chart(selected_columns, x_range, y_range)
                if chart:
                    st.pyplot(chart)
            else:
                st.warning("âš ï¸ è«‹é¸æ“‡è‡³å°‘ä¸€å€‹æ¬„ä½é€²è¡Œé¡¯ç¤º")
        
        else:
            # è‡ªå®šç¾©é›™è»¸åœ–è¡¨
            left_y_axis = st.sidebar.selectbox(
                "ğŸ“ˆ å·¦å´Yè»¸è®Šæ•¸", 
                options=numeric_columns, 
                index=0, 
                key=f"{key_prefix}left_y_axis"
            )
            
            right_y_axis_options = ['None'] + numeric_columns
            right_y_axis = st.sidebar.selectbox(
                "ğŸ“Š å³å´Yè»¸è®Šæ•¸ (å¯é¸)", 
                options=right_y_axis_options, 
                index=0, 
                key=f"{key_prefix}right_y_axis"
            )
            
            # ç”Ÿæˆåœ–è¡¨
            st.markdown("### ğŸ“Š æ–‡å­—æª”è‡ªå®šç¾©åœ–è¡¨")
            chart = self.chart_gen.generate_flexible_chart(
                self.log_data, left_y_axis, right_y_axis, x_range, None, None
            )
            if chart:
                st.pyplot(chart)
        
        # çµ±è¨ˆæ•¸æ“š
        st.markdown("### ğŸ“ˆ çµ±è¨ˆæ•¸æ“š")
        temp_stats = self.stats_calc.calculate_temp_stats(self.log_data, x_range)
        if not temp_stats.empty:
            st.dataframe(temp_stats, use_container_width=True, hide_index=True)
        
        # æ•¸æ“šé è¦½
        with st.expander("ğŸ” åŸå§‹æ•¸æ“šé è¦½", expanded=False):
            st.dataframe(self.log_data.df.head(20), use_container_width=True)
    
    def _generate_multi_line_chart(self, selected_columns: List[str], x_range, y_range=None):
        """ç”Ÿæˆå¤šç·šåœ–è¡¨"""
        try:
            df = self.log_data.filter_by_time(x_range)
            
            if df.empty or not selected_columns:
                return None
            
            available_columns = [col for col in selected_columns if col in df.columns]
            if not available_columns:
                return None
            
            fig, ax = plt.subplots(figsize=(12, 6))
            
            x_axis_seconds = df.index.total_seconds()
            colors = plt.cm.tab10(np.linspace(0, 1, len(available_columns)))
            
            for i, col in enumerate(available_columns):
                y_data = pd.to_numeric(df[col], errors='coerce')
                if not y_data.isna().all():
                    clean_name = col.replace('TXT: ', '')
                    ax.plot(x_axis_seconds, y_data, 
                           label=clean_name, 
                           color=colors[i], 
                           linewidth=1.5, 
                           alpha=0.8)
            
            ax.set_title("æ–‡å­—æª”æ•¸æ“šåœ–è¡¨", fontsize=14, fontweight='bold')
            ax.set_xlabel('Elapsed Time (seconds)', fontsize=11)
            ax.set_ylabel('Value', fontsize=11)
            ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
            
            if len(available_columns) <= 10:
                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
            else:
                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8, ncol=2)
            
            if x_range:
                ax.set_xlim(x_range)
            if y_range:
                ax.set_ylim(y_range)
            
            fig.tight_layout()
            return fig
            
        except Exception as e:
            st.error(f"åœ–è¡¨ç”Ÿæˆå¤±æ•—: {e}")
            return None

# =============================================================================
# å…¶ä»–æ¸²æŸ“å™¨é¡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
# =============================================================================

class YokogawaRenderer:
    """YOKOGAWA UIæ¸²æŸ“å™¨"""
    
    def __init__(self, log_data: LogData):
        self.log_data = log_data
        self.stats_calc = StatisticsCalculator()
        self.chart_gen = ChartGenerator()
    
    def render(self, file_index=None):
        st.markdown("### ğŸ“Š YOKOGAWA Log è§£æå®Œæˆï¼")
        st.success(f"ğŸ“Š æ•¸æ“šè¼‰å…¥ï¼š{self.log_data.metadata.rows} è¡Œ Ã— {self.log_data.metadata.columns} åˆ—")
        
        # ç°¡å–®çš„çµ±è¨ˆæ•¸æ“šé¡¯ç¤º
        temp_stats = self.stats_calc.calculate_temp_stats(self.log_data, None)
        if not temp_stats.empty:
            st.markdown("### ğŸ“ˆ çµ±è¨ˆæ•¸æ“š")
            st.dataframe(temp_stats, use_container_width=True, hide_index=True)

class BurnInRenderer:
    """Burn-in UIæ¸²æŸ“å™¨"""
    
    def __init__(self, log_data: LogData):
        self.log_data = log_data
        self.stats_calc = StatisticsCalculator()
    
    def render(self, file_index=None):
        st.markdown("### ğŸ”¥ Burn-in Log è§£æå®Œæˆï¼")
        st.success(f"ğŸ“Š æ•¸æ“šè¼‰å…¥ï¼š{self.log_data.metadata.rows} è¡Œ Ã— {self.log_data.metadata.columns} åˆ—")

class GPUMonRenderer:
    """GPUMon UIæ¸²æŸ“å™¨"""
    
    def __init__(self, log_data: LogData):
        self.log_data = log_data
        self.stats_calc = StatisticsCalculator()
    
    def render(self, file_index=None):
        st.markdown("### ğŸ® GPUMon Log è§£æå®Œæˆï¼")
        st.success(f"ğŸ“Š æ•¸æ“šè¼‰å…¥ï¼š{self.log_data.metadata.rows} è¡Œ Ã— {self.log_data.metadata.columns} åˆ—")

class PTATRenderer:
    """PTAT UIæ¸²æŸ“å™¨"""
    
    def __init__(self, log_data: LogData):
        self.log_data = log_data
        self.stats_calc = StatisticsCalculator()
    
    def render(self, file_index=None):
        st.markdown("### ğŸ–¥ï¸ PTAT Log è§£æå®Œæˆï¼")
        st.success(f"ğŸ“Š æ•¸æ“šè¼‰å…¥ï¼š{self.log_data.metadata.rows} è¡Œ Ã— {self.log_data.metadata.columns} åˆ—")

class SummaryRenderer:
    """Summary UIæ¸²æŸ“å™¨"""
    
    def __init__(self, log_data_list: List[LogData]):
        self.log_data_list = log_data_list
        self.summary_gen = TemperatureSummaryGenerator()
    
    def render(self):
        st.markdown("## ğŸ“‹ æ‰€æœ‰æª”æ¡ˆæº«åº¦æ•´åˆè¡¨æ ¼")
        
        summary_df = self.summary_gen.generate_summary_table(self.log_data_list)
        
        if summary_df.empty:
            st.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æº«åº¦æ•¸æ“š")
            return
        
        display_df = self.summary_gen.format_summary_table_for_display(summary_df)
        
        if not display_df.empty:
            st.markdown("### ğŸ“‹ æº«åº¦ç›£æ§é»æ•´åˆè¡¨æ ¼")
            st.dataframe(display_df, use_container_width=True, hide_index=True)

# =============================================================================
# UIå·¥å» 
# =============================================================================

class RendererFactory:
    """UIæ¸²æŸ“å™¨å·¥å»  - åŒ…å«æ–‡å­—æª”æ”¯æ´"""
    
    @staticmethod
    def create_renderer(log_data: LogData):
        """æ ¹æ“šlogé¡å‹å‰µå»ºå°æ‡‰çš„æ¸²æŸ“å™¨"""
        log_type = log_data.metadata.log_type
        
        if log_type == "Text/Log File":
            return TextFileRenderer(log_data)
        elif log_type == "Burn-in Log":
            return BurnInRenderer(log_data)
        elif log_type == "GPUMon Log":
            return GPUMonRenderer(log_data)
        elif log_type == "PTAT Log":
            return PTATRenderer(log_data)
        elif log_type == "YOKOGAWA Log":
            return YokogawaRenderer(log_data)
        else:
            return None

# =============================================================================
# ä¸»æ‡‰ç”¨ç¨‹å¼
# =============================================================================

def display_version_info():
    """é¡¯ç¤ºç‰ˆæœ¬è³‡è¨Š"""
    with st.expander("ğŸ“‹ ç‰ˆæœ¬è³‡è¨Š", expanded=False):
        st.markdown(f"""
        **ç•¶å‰ç‰ˆæœ¬ï¼š{VERSION}** | **ç™¼å¸ƒæ—¥æœŸï¼š{VERSION_DATE}**
        
        ### âœ¨ ä¸»è¦åŠŸèƒ½
        
        - **ğŸ“„ æ–‡å­—æª”æ”¯æ´** - .log, .txt, .dat ç­‰æ–‡å­—æ ¼å¼ç›´æ¥è§£æ
        - **ğŸ”¥ Burn-in Log** - ç‡’æ©Ÿæ¸¬è©¦æ•¸æ“šè§£æï¼ŒCPU Coreæº«åº¦é »ç‡ç›£æ§
        - **ğŸ® GPUMon Log** - GPUæ€§èƒ½ç›£æ§æ•¸æ“šè§£æèˆ‡è¦–è¦ºåŒ–
        - **ğŸ–¥ï¸ PTAT Log** - CPUæ€§èƒ½ç›£æ§æ•¸æ“šè§£æèˆ‡è¦–è¦ºåŒ–  
        - **ğŸ“Š YOKOGAWA Log** - å¤šé€šé“æº«åº¦è¨˜éŒ„å„€æ•¸æ“šè§£æèˆ‡è¦–è¦ºåŒ–
        - **ğŸ“‹ Summaryæ•´åˆ** - å¤šæª”æ¡ˆæº«åº¦æ•¸æ“šæ•´åˆï¼Œç”Ÿæˆå¸¶é‚Šæ¡†HTMLè¡¨æ ¼
        
        ### ğŸ“„ æ–‡å­—æª”æ–°åŠŸèƒ½
        
        - **æ™ºèƒ½è­˜åˆ¥** - è‡ªå‹•åˆ†ææ–‡å­—æª”æ ¼å¼å’Œåˆ†éš”ç¬¦
        - **å¤šç·¨ç¢¼æ”¯æ´** - UTF-8ã€GBKã€Big5ç­‰å¤šç¨®ç·¨ç¢¼
        - **éˆæ´»è§£æ** - è‡ªå‹•è­˜åˆ¥é€—è™Ÿã€Tabã€ç©ºæ ¼ç­‰åˆ†éš”ç¬¦
        - **æ™‚é–“è­˜åˆ¥** - æ™ºèƒ½è­˜åˆ¥å„ç¨®æ™‚é–“æ ¼å¼
        - **å³æ™‚åœ–è¡¨** - æ”¯æ´å¤šç·šåœ–è¡¨å’Œé›™è»¸åœ–è¡¨
        - **æ•¸å€¼è½‰æ›** - è‡ªå‹•è½‰æ›æ•¸å€¼å‹æ¬„ä½
        """)

def main():
    """ä¸»ç¨‹å¼ - åŒ…å«æ–‡å­—æª”æ”¯æ´"""
    st.set_page_config(
        page_title="æº«åº¦æ•¸æ“šè¦–è¦ºåŒ–å¹³å°",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # CSSæ¨£å¼
    st.markdown("""
    <style>
        .main-header {
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            padding: 1.5rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            text-align: center;
            color: white;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # æ¨™é¡Œ
    st.markdown(f"""
    <div class="main-header">
        <h1>ğŸ“Š æº«åº¦æ•¸æ“šè¦–è¦ºåŒ–å¹³å°</h1>
        <p>æ™ºèƒ½è§£æ æ–‡å­—æª”ã€YOKOGAWAã€PTATã€GPUMonã€Burn-in Log | å¤šæª”æ¡ˆç¨ç«‹åˆ†æ + Summaryæ•´åˆ</p>
        <p><strong>{VERSION}</strong> | {VERSION_DATE}</p>
    </div>
    """, unsafe_allow_html=True)
    
    display_version_info()
    
    # åˆå§‹åŒ–è§£æå™¨è¨»å†Šç³»çµ± - æ·»åŠ æ–‡å­—æª”è§£æå™¨
    parser_registry = ParserRegistry()
    parser_registry.register(TextFileParser())    # å„ªå…ˆè¨»å†Šæ–‡å­—æª”è§£æå™¨
    parser_registry.register(BurnInParser())
    parser_registry.register(GPUMonParser())
    parser_registry.register(PTATParser())
    parser_registry.register(YokogawaParser())    # å…œåº•è§£æå™¨
    
    # å´é‚Šæ¬„
    st.sidebar.markdown("### ğŸ›ï¸ æ§åˆ¶é¢æ¿")
    st.sidebar.markdown("---")
    
    # ä¿®æ”¹æ–‡ä»¶ä¸Šå‚³å™¨æ”¯æ´æ–‡å­—æª”
    uploaded_files = st.sidebar.file_uploader(
        "ğŸ“ ä¸Šå‚³Log File (å¯å¤šé¸)", 
        type=['csv', 'xlsx', 'log', 'txt', 'dat', 'out'],  # æ·»åŠ æ–‡å­—æª”æ ¼å¼
        accept_multiple_files=True,
        help="æ”¯æ´: æ–‡å­—æª”(.log/.txt)ã€Burn-inç‡’æ©Ÿæ¸¬è©¦ã€YOKOGAWAæº«åº¦è¨˜éŒ„ã€PTAT CPUç›£æ§ã€GPUMon GPUç›£æ§"
    )
    
    # é¡¯ç¤ºè¨ªå•è¨ˆæ•¸å™¨
    display_visit_counter()
    
    if uploaded_files:
        # é¡¯ç¤ºä¸Šå‚³æª”æ¡ˆè³‡è¨Š
        st.sidebar.markdown("---")
        st.sidebar.markdown("### ğŸ“‚ å·²ä¸Šå‚³æª”æ¡ˆ")
        for i, file in enumerate(uploaded_files, 1):
            file_size = len(file.getvalue()) / 1024
            st.sidebar.markdown(f"**{i}.** `{file.name}` ({file_size:.1f} KB)")
        
        st.sidebar.markdown("---")
        
        # è§£ææª”æ¡ˆ
        log_data_list = []
        for uploaded_file in uploaded_files:
            log_data = parser_registry.parse_file(uploaded_file)
            if log_data:
                log_data_list.append(log_data)
        
        if not log_data_list:
            st.error("âŒ ç„¡æ³•è§£æä»»ä½•æª”æ¡ˆ")
            return
        
        # æ ¹æ“šæª”æ¡ˆæ•¸é‡æ±ºå®šUIæ¨¡å¼
        if len(log_data_list) == 1:
            # å–®æª”æ¡ˆæ¨¡å¼
            log_data = log_data_list[0]
            renderer = RendererFactory.create_renderer(log_data)
            
            if renderer:
                renderer.render(file_index=0)
            else:
                st.error(f"ä¸æ”¯æ´çš„Logé¡å‹: {log_data.metadata.log_type}")
        
        else:
            # å¤šæª”æ¡ˆæ¨¡å¼
            st.success(f"ğŸ“Š å¤šæª”æ¡ˆåˆ†ææ¨¡å¼ï¼šæˆåŠŸè§£æ {len(log_data_list)} å€‹æª”æ¡ˆ")
            
            # å‰µå»ºæ¨™ç±¤é 
            tab_names = ["ğŸ“‹ Summary"]
            
            for i, log_data in enumerate(log_data_list):
                filename = log_data.metadata.filename
                log_type = log_data.metadata.log_type
                
                short_name = filename[:12] + "..." if len(filename) > 15 else filename
                
                if "Text/Log" in log_type:
                    tab_name = f"ğŸ“„ {short_name}"
                elif "Burn-in" in log_type:
                    tab_name = f"ğŸ”¥ {short_name}"
                elif "GPUMon" in log_type:
                    tab_name = f"ğŸ® {short_name}"
                elif "PTAT" in log_type:
                    tab_name = f"ğŸ–¥ï¸ {short_name}"
                elif "YOKOGAWA" in log_type:
                    tab_name = f"ğŸ“Š {short_name}"
                else:
                    tab_name = f"ğŸ“„ {short_name}"
                
                tab_names.append(tab_name)
            
            tabs = st.tabs(tab_names)
            
            # Summaryæ¨™ç±¤é 
            with tabs[0]:
                summary_renderer = SummaryRenderer(log_data_list)
                summary_renderer.render()
            
            # å€‹åˆ¥æª”æ¡ˆæ¨™ç±¤é 
            for i, (tab, log_data) in enumerate(zip(tabs[1:], log_data_list)):
                with tab:
                    st.markdown(f"""
                    <div style="background-color: #f0f8ff; padding: 1rem; border-radius: 8px; margin-bottom: 1rem; border-left: 4px solid #1f77b4;">
                        <h4>ğŸ“ æª”æ¡ˆè³‡è¨Š</h4>
                        <p><strong>æª”æ¡ˆåç¨±ï¼š</strong> {log_data.metadata.filename}</p>
                        <p><strong>æª”æ¡ˆé¡å‹ï¼š</strong> {log_data.metadata.log_type}</p>
                        <p><strong>æ•¸æ“šè¦æ¨¡ï¼š</strong> {log_data.metadata.rows} è¡Œ Ã— {log_data.metadata.columns} åˆ—</p>
                        <p><strong>æª”æ¡ˆå¤§å°ï¼š</strong> {log_data.metadata.file_size_kb:.1f} KB</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    renderer = RendererFactory.create_renderer(log_data)
                    
                    if renderer:
                        renderer.render(file_index=i)
                    else:
                        st.error(f"ä¸æ”¯æ´çš„Logé¡å‹: {log_data.metadata.log_type}")
    
    else:
        st.info("ğŸš€ **é–‹å§‹ä½¿ç”¨** - è«‹åœ¨å·¦å´ä¸Šå‚³æ‚¨çš„ Log æ–‡ä»¶é€²è¡Œåˆ†æ")
        
        st.markdown("""
        ### ğŸ“‹ æ”¯æ´çš„æª”æ¡ˆæ ¼å¼
        
        - **ğŸ“„ æ–‡å­—æª” (.log, .txt, .dat)** - ä»»ä½•åŒ…å«æ•¸æ“šçš„æ–‡å­—æ ¼å¼æª”æ¡ˆ â­ **æ–°åŠŸèƒ½**
        - **ğŸ”¥ Burn-in CSV/Excel** - ç‡’æ©Ÿæ¸¬è©¦æ•¸æ“šï¼ˆCPU Coreæº«åº¦ã€é »ç‡ç›£æ§ï¼‰
        - **ğŸ® GPUMon CSV** - GPUæ€§èƒ½ç›£æ§æ•¸æ“šï¼ˆæº«åº¦ã€åŠŸè€—ã€é »ç‡ã€ä½¿ç”¨ç‡ï¼‰
        - **ğŸ–¥ï¸ PTAT CSV** - CPUæ€§èƒ½ç›£æ§æ•¸æ“šï¼ˆé »ç‡ã€åŠŸè€—ã€æº«åº¦ï¼‰
        - **ğŸ“Š YOKOGAWA Excel/CSV** - å¤šé€šé“æº«åº¦è¨˜éŒ„å„€æ•¸æ“š
        
        ### âœ¨ æ–‡å­—æª”ç‰¹è‰²åŠŸèƒ½ â­
        
        - **ğŸ“„ æ™ºèƒ½è­˜åˆ¥** - è‡ªå‹•åˆ†ææ–‡å­—æª”æ ¼å¼å’Œåˆ†éš”ç¬¦
        - **ğŸ” å¤šç·¨ç¢¼æ”¯æ´** - æ”¯æ´UTF-8ã€GBKã€Big5ç­‰å¤šç¨®ç·¨ç¢¼
        - **âš¡ éˆæ´»è§£æ** - è‡ªå‹•è­˜åˆ¥é€—è™Ÿã€Tabã€ç©ºæ ¼ç­‰åˆ†éš”ç¬¦
        - **ğŸ¯ æ™‚é–“è­˜åˆ¥** - æ™ºèƒ½è­˜åˆ¥æ™‚é–“æ¬„ä½æ ¼å¼
        - **ğŸ“Š å³æ™‚åœ–è¡¨** - æ”¯æ´å¤šç·šåœ–è¡¨å’Œé›™è»¸åœ–è¡¨
        - **ğŸ”¢ æ•¸å€¼è½‰æ›** - è‡ªå‹•è½‰æ›æ•¸å€¼å‹æ¬„ä½
        
        ### ğŸ¯ ä½¿ç”¨æµç¨‹
        
        1. **ä¸Šå‚³æª”æ¡ˆ** - ç›´æ¥æ‹–æ‹½.logæˆ–.txtæª”æ¡ˆåˆ°å·¦å´ä¸Šå‚³å€
        2. **è‡ªå‹•è§£æ** - å¹³å°æœƒè‡ªå‹•è­˜åˆ¥æ–‡ä»¶æ ¼å¼å’Œæ•¸æ“šçµæ§‹
        3. **åœ–è¡¨åˆ†æ** - é¸æ“‡æ¬„ä½ç”Ÿæˆäº¤äº’å¼åœ–è¡¨
        4. **çµ±è¨ˆæ•¸æ“š** - æŸ¥çœ‹æœ€å¤§å€¼ã€æœ€å°å€¼ã€å¹³å‡å€¼ç­‰çµ±è¨ˆ
        5. **æ•´åˆå ±å‘Š** - åœ¨Summaryæ¨™ç±¤é æŸ¥çœ‹æ‰€æœ‰æª”æ¡ˆæ•´åˆçµæœ
        """)

if __name__ == "__main__":
    main()
